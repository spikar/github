{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251      @USER All you need to know is he is empty inside\n",
       "3720                            @USER I. AM. READ. E! URL\n",
       "8376    @USER Go roger I quit watching anyway nfl is o...\n",
       "4471               @USER Yoo our dogs should totally fuck\n",
       "2835              @USER He is a troll.  Not open to facts\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from torchtext import data\n",
    "#from torchtext import datasets\n",
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re # for regular expressions\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "data =  pd.read_csv('transferlearning-dl-spring2020/train.csv')\n",
    "\n",
    "data.head()\n",
    "\n",
    "questions = data['text']\n",
    "labels = data['target']\n",
    "\n",
    "train_data, valid_data, ytrain, yvalid = train_test_split(questions, labels,  \n",
    "                                                          random_state=42, \n",
    "                                                          test_size=0.2)\n",
    "\n",
    "train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97670</td>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  target\n",
       "0  86426  @USER She should ask a few native Americans wh...       1\n",
       "1  16820  Amazon is investigating Chinese employees who ...       0\n",
       "2  62688  @USER Someone should'veTaken\" this piece of sh...       1\n",
       "3  43605  @USER @USER Obama wanted liberals &amp; illega...       0\n",
       "4  97670                  @USER Liberals are all Kookoo !!!       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [She, should, ask, few, native, Americans, wha...\n",
       "1       [Amazon, investigating, Chinese, employee, who...\n",
       "2       [Someone, should, veTaken, this, piece, shit, ...\n",
       "3       [Obama, wanted, liberal, amp, illegals, move, ...\n",
       "4                            [Liberals, are, all, Kookoo]\n",
       "5       [wa, literally, just, talking, about, this, lo...\n",
       "6                                   [Buy, more, icecream]\n",
       "7                [not, fault, you, support, gun, control]\n",
       "8       [What, the, difference, between, Kavanaugh, an...\n",
       "9       [you, are, lying, corrupt, traitor, Nobody, wa...\n",
       "10      [like, soda, like, like, boarder, with, lot, ICE]\n",
       "11                     [you, are, also, the, king, taste]\n",
       "12      [MAGA, Sing, like, one, listening, Love, like,...\n",
       "13      [The, time, right, for, this, House, respond, ...\n",
       "14      [Besides, Jax, mom, and, maybe, Ope, hand, dow...\n",
       "15      [gun, control, That, all, these, kid, are, ask...\n",
       "16      [fuck, going, people, There, the, men, room, a...\n",
       "17      [Been, Willie, fan, since, before, most, you, ...\n",
       "18      [Tbh, these, day, just, don, like, people, gen...\n",
       "19      [South, Korean, Official, Leaders, will, discu...\n",
       "20                          [You, can, tell, hooper, too]\n",
       "21              [feel, like, better, chasing, the, title]\n",
       "22      [She, whom, are, you, referring, Hillary, You,...\n",
       "23      [Glad, see, your, friend, are, supporting, Met...\n",
       "24      [Yes, you, are, but, wa, asking, what, about, ...\n",
       "25      [wonder, being, apologetic, and, more, social,...\n",
       "26      [Any, update, ending, your, blatant, racism, W...\n",
       "27                                           [Blow, hard]\n",
       "28      [That, mean, you, are, max, lvl, Twitter, user...\n",
       "29      [Please, explain, what, controlled, opposition...\n",
       "                              ...                        \n",
       "9316       [Berkeley, Antifa, not, agree, with, you, URL]\n",
       "9317    [Advocate, for, gun, control, while, breaking,...\n",
       "9318    [More, press, should, talk, about, this, remar...\n",
       "9319    [MohammadHassanKhalid, traitor, the, USA, amp,...\n",
       "9320                       [nah, coz, you, cunt, blocked]\n",
       "9321                      [Cats, are, just, special, dog]\n",
       "9322    [Best, news, ever, for, GOP, Ready, the, strai...\n",
       "9323    [Strength, letting, the, universe, You, are, n...\n",
       "9324                                [Wow, You, are, good]\n",
       "9325    [still, lie, just, like, Obama, talk, right, a...\n",
       "9326                                           [precious]\n",
       "9327                                          [Booooring]\n",
       "9328    [Everything, else, wa, ten, year, ago, YOU, AR...\n",
       "9329                              [Right, Dang, She, the]\n",
       "9330    [McRaven, engaged, publicity, stunt, never, sp...\n",
       "9331    [man, You, are, going, trigger, the, fanboys, ...\n",
       "9332    [Chelsea, never, end, You, are, always, the, b...\n",
       "9333                                  [head, fuckin, egg]\n",
       "9334    [Alt, Right, amp, Antifa, are, for, coward, Fa...\n",
       "9335    [Did, you, serve, You, rate, bottom, the, barr...\n",
       "9336    [just, trying, make, good, with, his, libnut, ...\n",
       "9337    [have, the, conservative, accepted, the, antis...\n",
       "9338    [Can, all, agree, that, Tomlins, seat, heating...\n",
       "9339    [involved, because, wa, there, Now, need, man,...\n",
       "9340    [How, much, lonely, she, and, how, much, she, ...\n",
       "9341                                  [BUT, GUN, CONTROL]\n",
       "9342    [say, you, are, mad, now, you, will, say, tire...\n",
       "9343     [Retweet, complete, amp, followed, all, patriot]\n",
       "9344    [And, why, report, this, garbage, don, give, c...\n",
       "9345                                              [Pussy]\n",
       "Name: text, Length: 9346, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt    \n",
    "\n",
    "\n",
    "data['text'] = np.vectorize(remove_pattern)(data['text'], \"@[\\w]*\") \n",
    "\n",
    "\n",
    "data['text'] = data['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "data['text'] = data['text'].str.replace('#','')\n",
    "\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "\n",
    "\n",
    "tokenized_tweet = data['text'].apply(lambda x: x.split()) # tokenizing\n",
    "\n",
    "tokenized_tweet.head()\n",
    "\n",
    "len(tokenized_tweet)\n",
    "\n",
    "#from nltk.stem.porter import *\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "#tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_tweet.apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) # stemming\n",
    "#lemmatized_output = ' '.join([lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    [Amazon, investigating, Chinese, employees, wh...\n",
       "2    [Someone, should, veTaken, this, piece, shit, ...\n",
       "3    [Obama, wanted, liberals, amp, illegals, move,...\n",
       "4                         [Liberals, are, all, Kookoo]\n",
       "5    [was, literally, just, talking, about, this, l...\n",
       "6                                [Buy, more, icecream]\n",
       "7             [not, fault, you, support, gun, control]\n",
       "8    [What, the, difference, between, Kavanaugh, an...\n",
       "9    [you, are, lying, corrupt, traitor, Nobody, wa...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pre-trained word2vec embeddings created by Google\n",
    "\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "#model.train(tokenized_tweet, total_examples= len(combi['description']), epochs=50)\n",
    "\n",
    "def word_vector_pretrained(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 300))\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector_pretrained(tokenized_tweet[i], 300)\n",
    "    \n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape    \n",
    "\n",
    "X_train, X_test , y_train, y_test = train_test_split(wordvec_df, labels, test_size=0.2, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    [Amazon, investigating, Chinese, employees, wh...\n",
       "2    [Someone, should, veTaken, this, piece, shit, ...\n",
       "3    [Obama, wanted, liberals, amp, illegals, move,...\n",
       "4                         [Liberals, are, all, Kookoo]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5633</th>\n",
       "      <td>-0.006725</td>\n",
       "      <td>0.010405</td>\n",
       "      <td>-0.052097</td>\n",
       "      <td>0.091202</td>\n",
       "      <td>-0.059473</td>\n",
       "      <td>0.020924</td>\n",
       "      <td>0.077883</td>\n",
       "      <td>-0.019114</td>\n",
       "      <td>0.072357</td>\n",
       "      <td>0.102734</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086390</td>\n",
       "      <td>0.052601</td>\n",
       "      <td>-0.029518</td>\n",
       "      <td>0.016796</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.057052</td>\n",
       "      <td>0.016689</td>\n",
       "      <td>0.051769</td>\n",
       "      <td>0.050066</td>\n",
       "      <td>-0.035820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>0.029175</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.033051</td>\n",
       "      <td>0.113220</td>\n",
       "      <td>-0.112213</td>\n",
       "      <td>0.008484</td>\n",
       "      <td>0.028618</td>\n",
       "      <td>0.043793</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>-0.042809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010437</td>\n",
       "      <td>0.056587</td>\n",
       "      <td>-0.114441</td>\n",
       "      <td>-0.093079</td>\n",
       "      <td>0.005615</td>\n",
       "      <td>0.057098</td>\n",
       "      <td>-0.078369</td>\n",
       "      <td>-0.110870</td>\n",
       "      <td>0.012726</td>\n",
       "      <td>0.060181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>0.033418</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>0.030201</td>\n",
       "      <td>0.066354</td>\n",
       "      <td>-0.040874</td>\n",
       "      <td>0.010851</td>\n",
       "      <td>0.012351</td>\n",
       "      <td>-0.074940</td>\n",
       "      <td>0.095403</td>\n",
       "      <td>0.074498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059332</td>\n",
       "      <td>0.024525</td>\n",
       "      <td>-0.122869</td>\n",
       "      <td>0.023648</td>\n",
       "      <td>-0.009204</td>\n",
       "      <td>-0.021667</td>\n",
       "      <td>0.024586</td>\n",
       "      <td>-0.097644</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>-0.051945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>0.017432</td>\n",
       "      <td>0.039795</td>\n",
       "      <td>-0.022937</td>\n",
       "      <td>-0.007690</td>\n",
       "      <td>0.017090</td>\n",
       "      <td>-0.051917</td>\n",
       "      <td>0.034424</td>\n",
       "      <td>-0.123288</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>0.031116</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018947</td>\n",
       "      <td>-0.060611</td>\n",
       "      <td>-0.126147</td>\n",
       "      <td>-0.063501</td>\n",
       "      <td>-0.153271</td>\n",
       "      <td>-0.098695</td>\n",
       "      <td>0.072009</td>\n",
       "      <td>-0.020317</td>\n",
       "      <td>0.141699</td>\n",
       "      <td>-0.069684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>0.023802</td>\n",
       "      <td>0.033569</td>\n",
       "      <td>0.142127</td>\n",
       "      <td>0.134559</td>\n",
       "      <td>-0.098013</td>\n",
       "      <td>-0.016130</td>\n",
       "      <td>0.057371</td>\n",
       "      <td>-0.051627</td>\n",
       "      <td>0.050809</td>\n",
       "      <td>0.048138</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015437</td>\n",
       "      <td>0.078177</td>\n",
       "      <td>-0.058939</td>\n",
       "      <td>-0.020576</td>\n",
       "      <td>-0.057497</td>\n",
       "      <td>0.038525</td>\n",
       "      <td>-0.050711</td>\n",
       "      <td>-0.067130</td>\n",
       "      <td>-0.025423</td>\n",
       "      <td>-0.003723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6560</th>\n",
       "      <td>0.061476</td>\n",
       "      <td>0.052678</td>\n",
       "      <td>0.018585</td>\n",
       "      <td>0.084633</td>\n",
       "      <td>-0.116528</td>\n",
       "      <td>-0.019473</td>\n",
       "      <td>-0.007350</td>\n",
       "      <td>-0.011179</td>\n",
       "      <td>0.071787</td>\n",
       "      <td>0.027387</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034809</td>\n",
       "      <td>0.024897</td>\n",
       "      <td>-0.082077</td>\n",
       "      <td>0.004482</td>\n",
       "      <td>-0.072059</td>\n",
       "      <td>-0.026894</td>\n",
       "      <td>0.036138</td>\n",
       "      <td>-0.113395</td>\n",
       "      <td>-0.018890</td>\n",
       "      <td>0.024719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>0.066352</td>\n",
       "      <td>0.058706</td>\n",
       "      <td>0.082484</td>\n",
       "      <td>0.095835</td>\n",
       "      <td>-0.066877</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.020893</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.003379</td>\n",
       "      <td>0.077595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090912</td>\n",
       "      <td>0.021102</td>\n",
       "      <td>-0.071148</td>\n",
       "      <td>-0.012076</td>\n",
       "      <td>0.004750</td>\n",
       "      <td>-0.002641</td>\n",
       "      <td>0.028205</td>\n",
       "      <td>-0.054700</td>\n",
       "      <td>0.011422</td>\n",
       "      <td>0.017927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6211</th>\n",
       "      <td>0.007294</td>\n",
       "      <td>-0.013245</td>\n",
       "      <td>-0.009155</td>\n",
       "      <td>-0.039429</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.053101</td>\n",
       "      <td>-0.025269</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.051147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229492</td>\n",
       "      <td>-0.023682</td>\n",
       "      <td>-0.151245</td>\n",
       "      <td>-0.058350</td>\n",
       "      <td>-0.083496</td>\n",
       "      <td>-0.065781</td>\n",
       "      <td>0.200684</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>-0.018311</td>\n",
       "      <td>-0.082153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>0.061823</td>\n",
       "      <td>-0.040497</td>\n",
       "      <td>0.053802</td>\n",
       "      <td>0.194775</td>\n",
       "      <td>-0.059338</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>0.075983</td>\n",
       "      <td>-0.038326</td>\n",
       "      <td>0.097266</td>\n",
       "      <td>0.041626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051514</td>\n",
       "      <td>-0.032813</td>\n",
       "      <td>-0.154071</td>\n",
       "      <td>-0.008032</td>\n",
       "      <td>-0.086324</td>\n",
       "      <td>-0.046680</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>-0.071629</td>\n",
       "      <td>-0.000305</td>\n",
       "      <td>-0.024475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "5633 -0.006725  0.010405 -0.052097  0.091202 -0.059473  0.020924  0.077883   \n",
       "4507  0.029175  0.037109  0.033051  0.113220 -0.112213  0.008484  0.028618   \n",
       "3891  0.033418  0.003791  0.030201  0.066354 -0.040874  0.010851  0.012351   \n",
       "5429  0.017432  0.039795 -0.022937 -0.007690  0.017090 -0.051917  0.034424   \n",
       "1104  0.023802  0.033569  0.142127  0.134559 -0.098013 -0.016130  0.057371   \n",
       "6560  0.061476  0.052678  0.018585  0.084633 -0.116528 -0.019473 -0.007350   \n",
       "3332  0.066352  0.058706  0.082484  0.095835 -0.066877  0.002801  0.020893   \n",
       "6211  0.007294 -0.013245 -0.009155 -0.039429  0.000488  0.053101 -0.025269   \n",
       "2364  0.061823 -0.040497  0.053802  0.194775 -0.059338  0.013220  0.075983   \n",
       "\n",
       "           7         8         9      ...          290       291       292  \\\n",
       "5633 -0.019114  0.072357  0.102734    ...    -0.086390  0.052601 -0.029518   \n",
       "4507  0.043793  0.001225 -0.042809    ...     0.010437  0.056587 -0.114441   \n",
       "3891 -0.074940  0.095403  0.074498    ...    -0.059332  0.024525 -0.122869   \n",
       "5429 -0.123288  0.058185  0.031116    ...    -0.018947 -0.060611 -0.126147   \n",
       "1104 -0.051627  0.050809  0.048138    ...    -0.015437  0.078177 -0.058939   \n",
       "6560 -0.011179  0.071787  0.027387    ...    -0.034809  0.024897 -0.082077   \n",
       "3332  0.003180  0.003379  0.077595    ...    -0.090912  0.021102 -0.071148   \n",
       "6211  0.000488  0.074219  0.051147    ...    -0.229492 -0.023682 -0.151245   \n",
       "2364 -0.038326  0.097266  0.041626    ...    -0.051514 -0.032813 -0.154071   \n",
       "\n",
       "           293       294       295       296       297       298       299  \n",
       "5633  0.016796  0.007548  0.057052  0.016689  0.051769  0.050066 -0.035820  \n",
       "4507 -0.093079  0.005615  0.057098 -0.078369 -0.110870  0.012726  0.060181  \n",
       "3891  0.023648 -0.009204 -0.021667  0.024586 -0.097644  0.018408 -0.051945  \n",
       "5429 -0.063501 -0.153271 -0.098695  0.072009 -0.020317  0.141699 -0.069684  \n",
       "1104 -0.020576 -0.057497  0.038525 -0.050711 -0.067130 -0.025423 -0.003723  \n",
       "6560  0.004482 -0.072059 -0.026894  0.036138 -0.113395 -0.018890  0.024719  \n",
       "3332 -0.012076  0.004750 -0.002641  0.028205 -0.054700  0.011422  0.017927  \n",
       "6211 -0.058350 -0.083496 -0.065781  0.200684 -0.036377 -0.018311 -0.082153  \n",
       "2364 -0.008032 -0.086324 -0.046680  0.002306 -0.071629 -0.000305 -0.024475  \n",
       "\n",
       "[9 rows x 300 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6691278341911253"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LogisticRegression(random_state = 10)\n",
    "lreg.fit(X_train, y_train) # training the model\n",
    "\n",
    "prediction = lreg.predict(X_test) # predicting on the validation set\n",
    "\n",
    "prediction_int = prediction.astype(np.int)\n",
    "\n",
    "f1_score(y_test, prediction,average = 'macro') # calculating f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test =  pd.read_csv('transferlearning-dl-spring2020/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    [home, you, drunk, MAGA, Trump, URL]\n",
       "1                                       [no, Tough, shit]\n",
       "2       [Canada, doesn, need, another, CUCK, already, ...\n",
       "3       [should, scare, every, American, She, playing,...\n",
       "4       [LOL, Throwing, the, BULLSHIT, Flag, such, non...\n",
       "5                                     [You, are, correct]\n",
       "6       [Kind, like, when, conservative, wanna, associ...\n",
       "7       [The, only, thing, the, Democrats, have, lying...\n",
       "8       [you, know, what, going, happen, now, going, h...\n",
       "9       [You, are, not, very, smart, are, you, Why, yo...\n",
       "10      [That, expected, you, placate, the, violent, l...\n",
       "11      [Wow, you, liberal, really, don, have, sense, ...\n",
       "12                                       [FUCKING, READY]\n",
       "13      [too, Her, wedding, wa, the, best, She, very, ...\n",
       "14      [URL, any, your, announcement, every, come, fr...\n",
       "15      [out, British, people, are, basically, full, r...\n",
       "16      [GUNCONTROL, advocate, must, STOP, falling, al...\n",
       "17                                            [Fuck, off]\n",
       "18      [dumb, and, dumber, all, one, president, two, ...\n",
       "19      [did, Twitter, silence, alex, jones, retaliati...\n",
       "20                   [Thats, because, you, are, old, man]\n",
       "21      [She, beautiful, person, teach, value, her, ki...\n",
       "22                      [tear, made, water, and, feeling]\n",
       "23      [babysitting, kid, people, how, old, you, thin...\n",
       "24      [WTC, This, threat, Maga, QAnon, WakeUpAmerica...\n",
       "25      [Unfortunately, America, system, like, that, w...\n",
       "26        [Guy, you, are, always, present, bet, why, Lol]\n",
       "27      [MAGA, YOU, ARE, ALL, FOR, TRUMP, FOLLOW, AND,...\n",
       "28      [For, the, record, know, Doug, Jones, personal...\n",
       "29      [There, are, many, dumb, argument, for, gun, c...\n",
       "                              ...                        \n",
       "3864    [Probably, while, screaming, about, gun, control]\n",
       "3865    [Thinking, about, the, Student, Loan, Crisis, ...\n",
       "3866    [the, tying, increase, the, cost, drug, Canada...\n",
       "3867    [Damn, straight, Nothing, but, good, vibe, com...\n",
       "3868    [THIS, SHOULD, REMIND, ALL, Patriots, maga, WH...\n",
       "3869    [sorry, what, BAG, listening, that, amp, after...\n",
       "3870    [Americans, make, great, client, But, many, ap...\n",
       "3871    [You, should, know, better, use, common, sense...\n",
       "3872                   [Keep, lying, bud, one, listening]\n",
       "3873           [You, are, great, model, for, inspiration]\n",
       "3874    [Chris, Chris, Chris, Are, you, forgetting, th...\n",
       "3875        [Where, will, Antifa, get, their, cloth, now]\n",
       "3876              [She, role, model, Adam, you, are, not]\n",
       "3877    [The, entire, way, the, dems, have, handled, t...\n",
       "3878    [Beto, Rourke, Ted, Cruz, Latest, Polls, Democ...\n",
       "3879    [Project, much, you, Every, campaign, flier, g...\n",
       "3880    [Link, and, knuckle, then, you, going, dodge, ...\n",
       "3881    [Omg, not, even, interested, his, age, but, da...\n",
       "3882                      [shit, you, weren, joking, wtf]\n",
       "3883    [Bibi, look, like, Stalin, when, Stalin, wa, y...\n",
       "3884    [right, say, that, housing, association, shoul...\n",
       "3885    [Boise, State, fan, can, tell, you, two, thing...\n",
       "3886    [advocating, for, conduct, within, bound, Huma...\n",
       "3887                            [when, you, coming, ohio]\n",
       "3888    [Liars, like, the, Antifa, twin, you, vigorous...\n",
       "3889    [Billy, you, have, short, memory, Obama, tried...\n",
       "3890         [She, not, the, brightest, light, the, tree]\n",
       "3891    [Sometimes, get, strong, vibe, from, people, a...\n",
       "3892    [Benidorm, Creamfields, Maga, Not, too, shabby...\n",
       "3893    [Spanishrevenge, justice, HumanRights, and, Fr...\n",
       "Name: text, Length: 3894, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['text'] = np.vectorize(remove_pattern)(data_test['text'], \"@[\\w]*\") \n",
    "\n",
    "\n",
    "data_test['text'] = data_test['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "data_test['text'] = data_test['text'].str.replace('#','')\n",
    "\n",
    "\n",
    "data_test['text'] = data_test['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "\n",
    "\n",
    "tokenized_tweet_test = data_test['text'].apply(lambda x: x.split()) # tokenizing\n",
    "\n",
    "tokenized_tweet_test.head()\n",
    "\n",
    "len(tokenized_tweet_test)\n",
    "\n",
    "#from nltk.stem.porter import *\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "#tokenized_tweet_test = tokenized_tweet_test.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_tweet_test.apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) # stemming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3894, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "wordvec_arrays_test = np.zeros((len(tokenized_tweet_test), 300))\n",
    "\n",
    "for i in range(len(tokenized_tweet_test)):\n",
    "    wordvec_arrays_test[i,:] = word_vector_pretrained(tokenized_tweet_test[i], 300)\n",
    "    \n",
    "wordvec_df_test = pd.DataFrame(wordvec_arrays_test)\n",
    "wordvec_df_test.shape    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = np.array(wordvec_df_test)\n",
    "\n",
    "prediction1 = lreg.predict(X_test1) # predicting on the validation set\n",
    "\n",
    "prediction_int1 = prediction1.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_int1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67757</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14726</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>74477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>49845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>47311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75689</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>84102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>98992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>53264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>54842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>48995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>72353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50759</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>93119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>43133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>59537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32317</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>76680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>80561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>54190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>28037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3866</th>\n",
       "      <td>77430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3867</th>\n",
       "      <td>75815</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>87290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>99475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>43323</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>37666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>77905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>83400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>84081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875</th>\n",
       "      <td>71649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>20841</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>90959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>32598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>43964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>97745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>86716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>21033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>66832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>28996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3885</th>\n",
       "      <td>64713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>63482</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3887</th>\n",
       "      <td>11132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3888</th>\n",
       "      <td>87416</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3889</th>\n",
       "      <td>90041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>98824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>95338</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3892</th>\n",
       "      <td>67210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893</th>\n",
       "      <td>46552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3894 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  Target\n",
       "0     90194       0\n",
       "1     77444       1\n",
       "2     13384       0\n",
       "3     54920       1\n",
       "4     56117       1\n",
       "5     67757       0\n",
       "6     12681       0\n",
       "7     12609       0\n",
       "8     70380       0\n",
       "9     12108       0\n",
       "10    14726       1\n",
       "11    74477       0\n",
       "12    49845       0\n",
       "13    47311       0\n",
       "14    75689       0\n",
       "15    84102       0\n",
       "16    10607       0\n",
       "17    98992       1\n",
       "18    53264       1\n",
       "19    54842       0\n",
       "20    48995       0\n",
       "21    72353       0\n",
       "22    50759       1\n",
       "23    14574       0\n",
       "24    93119       0\n",
       "25    43133       1\n",
       "26    59537       0\n",
       "27    32317       0\n",
       "28    76680       0\n",
       "29    80561       0\n",
       "...     ...     ...\n",
       "3864  54190       0\n",
       "3865  28037       0\n",
       "3866  77430       0\n",
       "3867  75815       0\n",
       "3868  87290       0\n",
       "3869  99475       0\n",
       "3870  43323       0\n",
       "3871  37666       0\n",
       "3872  77905       0\n",
       "3873  83400       0\n",
       "3874  84081       0\n",
       "3875  71649       0\n",
       "3876  20841       0\n",
       "3877  90959       0\n",
       "3878  32598       0\n",
       "3879  43964       0\n",
       "3880  97745       0\n",
       "3881  86716       0\n",
       "3882  21033       1\n",
       "3883  66832       0\n",
       "3884  28996       0\n",
       "3885  64713       0\n",
       "3886  63482       0\n",
       "3887  11132       0\n",
       "3888  87416       0\n",
       "3889  90041       0\n",
       "3890  98824       0\n",
       "3891  95338       0\n",
       "3892  67210       0\n",
       "3893  46552       0\n",
       "\n",
       "[3894 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['Target'] = prediction_int1\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('pretrained_word2vec.csv', index=False) # writing data to a CSV file\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained word2vec embeddings 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pre-trained word2vec embeddings created by Google\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"glove-twitter-200\")\n",
    "\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "#model.train(tokenized_tweet, total_examples= len(combi['description']), epochs=50)\n",
    "\n",
    "def word_vector_pretrained(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector_pretrained(tokenized_tweet[i], 200)\n",
    "    \n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape    \n",
    "\n",
    "X_train, X_test , y_train, y_test = train_test_split(wordvec_df, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6552467092841944"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LogisticRegression(random_state = 10)\n",
    "lreg.fit(X_train, y_train) # training the model\n",
    "\n",
    "prediction = lreg.predict(X_test) # predicting on the validation set\n",
    "\n",
    "prediction_int = prediction.astype(np.int)\n",
    "\n",
    "f1_score(y_test, prediction,average = 'macro') # calculating f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67757</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14726</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>74477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>49845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>47311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75689</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>84102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>98992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>53264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>54842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>48995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>72353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50759</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>93119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>43133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>59537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32317</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>76680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>80561</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>54190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>28037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3866</th>\n",
       "      <td>77430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3867</th>\n",
       "      <td>75815</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>87290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>99475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>43323</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>37666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>77905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>83400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>84081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875</th>\n",
       "      <td>71649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>20841</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>90959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>32598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>43964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>97745</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>86716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>21033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>66832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>28996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3885</th>\n",
       "      <td>64713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>63482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3887</th>\n",
       "      <td>11132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3888</th>\n",
       "      <td>87416</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3889</th>\n",
       "      <td>90041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>98824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>95338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3892</th>\n",
       "      <td>67210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893</th>\n",
       "      <td>46552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3894 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  Target\n",
       "0     90194       0\n",
       "1     77444       1\n",
       "2     13384       0\n",
       "3     54920       0\n",
       "4     56117       1\n",
       "5     67757       0\n",
       "6     12681       0\n",
       "7     12609       0\n",
       "8     70380       0\n",
       "9     12108       0\n",
       "10    14726       1\n",
       "11    74477       0\n",
       "12    49845       0\n",
       "13    47311       0\n",
       "14    75689       0\n",
       "15    84102       0\n",
       "16    10607       0\n",
       "17    98992       1\n",
       "18    53264       1\n",
       "19    54842       0\n",
       "20    48995       0\n",
       "21    72353       0\n",
       "22    50759       0\n",
       "23    14574       0\n",
       "24    93119       0\n",
       "25    43133       1\n",
       "26    59537       0\n",
       "27    32317       0\n",
       "28    76680       0\n",
       "29    80561       1\n",
       "...     ...     ...\n",
       "3864  54190       0\n",
       "3865  28037       0\n",
       "3866  77430       0\n",
       "3867  75815       0\n",
       "3868  87290       0\n",
       "3869  99475       0\n",
       "3870  43323       0\n",
       "3871  37666       0\n",
       "3872  77905       0\n",
       "3873  83400       0\n",
       "3874  84081       0\n",
       "3875  71649       1\n",
       "3876  20841       0\n",
       "3877  90959       0\n",
       "3878  32598       0\n",
       "3879  43964       0\n",
       "3880  97745       1\n",
       "3881  86716       0\n",
       "3882  21033       1\n",
       "3883  66832       0\n",
       "3884  28996       0\n",
       "3885  64713       0\n",
       "3886  63482       1\n",
       "3887  11132       0\n",
       "3888  87416       0\n",
       "3889  90041       0\n",
       "3890  98824       0\n",
       "3891  95338       1\n",
       "3892  67210       0\n",
       "3893  46552       0\n",
       "\n",
       "[3894 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test =  pd.read_csv('transferlearning-dl-spring2020/test.csv')\n",
    "\n",
    "data_test['text'] = np.vectorize(remove_pattern)(data_test['text'], \"@[\\w]*\") \n",
    "\n",
    "\n",
    "data_test['text'] = data_test['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "data_test['text'] = data_test['text'].str.replace('#','')\n",
    "\n",
    "\n",
    "data_test['text'] = data_test['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "\n",
    "\n",
    "tokenized_tweet_test = data_test['text'].apply(lambda x: x.split()) # tokenizing\n",
    "\n",
    "tokenized_tweet_test.head()\n",
    "\n",
    "len(tokenized_tweet_test)\n",
    "\n",
    "#from nltk.stem.porter import *\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "#tokenized_tweet_test = tokenized_tweet_test.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_tweet_test.apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) # stemming\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wordvec_arrays_test = np.zeros((len(tokenized_tweet_test), 200))\n",
    "\n",
    "for i in range(len(tokenized_tweet_test)):\n",
    "    wordvec_arrays_test[i,:] = word_vector_pretrained(tokenized_tweet_test[i], 200)\n",
    "    \n",
    "wordvec_df_test = pd.DataFrame(wordvec_arrays_test)\n",
    "wordvec_df_test.shape    \n",
    "\n",
    "\n",
    "\n",
    "X_test1 = np.array(wordvec_df_test)\n",
    "\n",
    "prediction1 = lreg.predict(X_test1) # predicting on the validation set\n",
    "\n",
    "prediction_int1 = prediction1.astype(np.int)\n",
    "\n",
    "prediction_int1\n",
    "\n",
    "data_test['Target'] = prediction_int1\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('pretrained_word2vec1.csv', index=False) # writing data to a CSV file\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuned word2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9346, 50)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=50, # desired no. of features/independent variables \n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34)\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(tokenized_tweet), epochs=200)\n",
    "\n",
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 50))\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 50)\n",
    "    \n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5291078614322713"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(wordvec_df, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "lreg = LogisticRegression(random_state = 20)\n",
    "lreg.fit(X_train, y_train) # training the model\n",
    "\n",
    "prediction = lreg.predict(X_test) # predicting on the validation set\n",
    "\n",
    "prediction_int = prediction.astype(np.int)\n",
    "\n",
    "f1_score(y_test, prediction,average = 'macro') # calculating f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for text classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_data_df_small = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [she, should, ask, few, native, americans, wha...\n",
      "1    [amazon, investigating, chinese, employees, wh...\n",
      "2    [someone, should, vetaken, this, piece, shit, ...\n",
      "3    [obama, wanted, liberals, amp, illegals, move,...\n",
      "4                         [liberals, are, all, kookoo]\n",
      "5    [was, literally, just, talking, about, this, l...\n",
      "6                                [buy, more, icecream]\n",
      "7             [not, fault, you, support, gun, control]\n",
      "8    [what, the, difference, between, kavanaugh, an...\n",
      "9    [you, are, lying, corrupt, traitor, nobody, wa...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "top_data_df_small['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in top_data_df_small['text']] \n",
    "print(top_data_df_small['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [she, should, ask, few, nativ, american, what,...\n",
       "1    [amazon, investig, chines, employe, who, ar, s...\n",
       "2    [someon, should, vetaken, thi, piec, shit, vol...\n",
       "3    [obama, want, liber, amp, illeg, move, into, r...\n",
       "4                             [liber, ar, all, kookoo]\n",
       "5    [wa, liter, just, talk, about, thi, lol, all, ...\n",
       "6                                [bui, more, icecream]\n",
       "7             [not, fault, you, support, gun, control]\n",
       "8    [what, the, differ, between, kavanaugh, and, o...\n",
       "9    [you, ar, ly, corrupt, traitor, nobodi, want, ...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "# Get the stemmed_tokens\n",
    "top_data_df_small['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in top_data_df_small['tokenized_text'] ]\n",
    "top_data_df_small['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for Train sentiments\n",
      "0    4365\n",
      "1    2177\n",
      "Name: target, dtype: int64\n",
      "Value counts for Test sentiments\n",
      "0    1855\n",
      "1     949\n",
      "Name: target, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "   index                                               text  \\\n",
      "0   8030                          least you are honest lmao   \n",
      "1   3722                         All that shit was hers URL   \n",
      "2   3522                         Cutie connection authentic   \n",
      "3   4232  What interview Evanne honest and right from th...   \n",
      "4   1678  Warriorscoach fundraising for gun control Brad...   \n",
      "\n",
      "                                      stemmed_tokens  \n",
      "0                     [least, you, ar, honest, lmao]  \n",
      "1                    [all, that, shit, wa, her, url]  \n",
      "2                           [cuti, connect, authent]  \n",
      "3  [what, interview, evann, honest, and, right, f...  \n",
      "4  [warriorscoach, fundrais, for, gun, control, b...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split Function\n",
    "def split_train_test(top_data_df_small, test_size=0.3, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[[ 'text',  'stemmed_tokens']], \n",
    "                                                        top_data_df_small['target'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train sentiments\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test sentiments\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Call the train_test_split\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "# Use cuda if present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9346\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "size = 500\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "\n",
    "# Function to train word2vec model\n",
    "def make_word2vec_model(top_data_df_small, padding=True, sg=1, min_count=1, size=500, workers=3, window=3):\n",
    "    if  padding:\n",
    "        print(len(top_data_df_small))\n",
    "        temp_df = pd.Series(top_data_df_small['stemmed_tokens']).values\n",
    "        temp_df = list(temp_df)\n",
    "        temp_df.append(['pad'])\n",
    "        word2vec_file =   'word2vec_' + str(size) + '_PAD.model'\n",
    "    else:\n",
    "        temp_df = top_data_df_small['stemmed_tokens']\n",
    "        word2vec_file =   'word2vec_' + str(size) + '.model'\n",
    "    w2v_model = Word2Vec(temp_df, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
    "\n",
    "    w2v_model.save(word2vec_file)\n",
    "    return w2v_model, word2vec_file\n",
    "\n",
    "# Train Word2vec model\n",
    "w2vmodel, word2vec_file = make_word2vec_model(top_data_df_small, padding=True, sg=sg, min_count=min_count, size=size, workers=workers, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the output tensor\n",
    "def make_target(label):\n",
    "    if label == -1:\n",
    "        return torch.tensor([0], dtype=torch.long, device=device)\n",
    "    elif label == 0:\n",
    "        return torch.tensor([1], dtype=torch.long, device=device)\n",
    "    else:\n",
    "        return torch.tensor([2], dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 500\n",
    "NUM_FILTERS = 10\n",
    "import gensim\n",
    "\n",
    "class CnnTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n",
    "        super(CnnTextClassifier, self).__init__()\n",
    "        w2vmodel = gensim.models.KeyedVectors.load( 'word2vec_500_PAD.model')\n",
    "        weights = w2vmodel.wv\n",
    "        # With pretrained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors), padding_idx=w2vmodel.wv.vocab['pad'].index)\n",
    "        # Without pretrained embeddings\n",
    "        # self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE], padding=(window_size - 1, 0))\n",
    "                                   for window_size in window_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # [B, T, E]\n",
    "\n",
    "        # Apply a convolution + max_pool layer for each window size\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        xs = []\n",
    "        for conv in self.convs:\n",
    "            x2 = torch.tanh(conv(x))\n",
    "            x2 = torch.squeeze(x2, -1)\n",
    "            x2 = F.max_pool1d(x2, x2.size(2))\n",
    "            xs.append(x2)\n",
    "        x = torch.cat(xs, 2)\n",
    "\n",
    "        # FC\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sen_len = top_data_df_small.stemmed_tokens.map(len).max()\n",
    "padding_idx = w2vmodel.wv.vocab['pad'].index\n",
    "def make_word2vec_vector_cnn(sentence):\n",
    "    padded_X = [padding_idx for i in range(max_sen_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        if word not in w2vmodel.wv.vocab:\n",
    "            padded_X[i] = 0\n",
    "            print(word)\n",
    "        else:\n",
    "            padded_X[i] = w2vmodel.wv.vocab[word].index\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n",
      "Epoch ran :1\n",
      "Epoch2\n",
      "Epoch ran :2\n",
      "Epoch3\n",
      "Epoch ran :3\n",
      "Epoch4\n",
      "Epoch ran :4\n",
      "Epoch5\n",
      "Epoch ran :5\n",
      "Epoch6\n",
      "Epoch ran :6\n",
      "Epoch7\n",
      "Epoch ran :7\n",
      "Epoch8\n",
      "Epoch ran :8\n",
      "Epoch9\n",
      "Epoch ran :9\n",
      "Epoch10\n",
      "Epoch ran :10\n",
      "Epoch11\n",
      "Epoch ran :11\n",
      "Epoch12\n",
      "Epoch ran :12\n",
      "Epoch13\n",
      "Epoch ran :13\n",
      "Epoch14\n",
      "Epoch ran :14\n",
      "Epoch15\n",
      "Epoch ran :15\n",
      "Epoch16\n",
      "Epoch ran :16\n",
      "Epoch17\n",
      "Epoch ran :17\n",
      "Epoch18\n",
      "Epoch ran :18\n",
      "Epoch19\n",
      "Epoch ran :19\n",
      "Epoch20\n",
      "Epoch ran :20\n",
      "Epoch21\n",
      "Epoch ran :21\n",
      "Epoch22\n",
      "Epoch ran :22\n",
      "Epoch23\n",
      "Epoch ran :23\n",
      "Epoch24\n",
      "Epoch ran :24\n",
      "Epoch25\n",
      "Epoch ran :25\n",
      "Epoch26\n",
      "Epoch ran :26\n",
      "Epoch27\n",
      "Epoch ran :27\n",
      "Epoch28\n",
      "Epoch ran :28\n",
      "Epoch29\n",
      "Epoch ran :29\n",
      "Epoch30\n",
      "Epoch ran :30\n",
      "Input vector\n",
      "[[  235    13    16     0   928   172   358   244     7     7 10887 10887\n",
      "  10887 10887 10887 10887 10887 10887 10887 10887 10887 10887 10887 10887\n",
      "  10887 10887 10887 10887 10887 10887 10887 10887 10887 10887 10887 10887\n",
      "  10887 10887 10887 10887 10887 10887 10887 10887 10887 10887 10887 10887\n",
      "  10887]]\n",
      "Probs\n",
      "tensor([[1.1411e-11, 9.5633e-01, 4.3666e-02]], grad_fn=<SoftmaxBackward>)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 3\n",
    "VOCAB_SIZE = len(w2vmodel.wv.vocab)\n",
    "\n",
    "cnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\n",
    "cnn_model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "num_epochs = 30\n",
    "\n",
    "# Open the file for writing loss\n",
    "loss_file_name = 'cnn_class_big_loss_with_padding.csv'\n",
    "f = open(loss_file_name,'w')\n",
    "f.write('iter, loss')\n",
    "f.write('\\n')\n",
    "losses = []\n",
    "cnn_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch\" + str(epoch + 1))\n",
    "    train_loss = 0\n",
    "    for index, row in X_train.iterrows():\n",
    "        # Clearing the accumulated gradients\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        # Make the bag of words vector for stemmed tokens \n",
    "        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n",
    "       \n",
    "        # Forward pass to get output\n",
    "        probs = cnn_model(bow_vec)\n",
    "\n",
    "        # Get the target label\n",
    "        target = make_target(Y_train['target'][index])\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = loss_function(probs, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # if index == 0:\n",
    "    #     continue\n",
    "    print(\"Epoch ran :\"+ str(epoch+1))\n",
    "    f.write(str((epoch+1)) + \",\" + str(train_loss / len(X_train)))\n",
    "    f.write('\\n')\n",
    "    train_loss = 0\n",
    "\n",
    "torch.save(cnn_model, 'cnn_big_model_500_with_padding.pth')\n",
    "\n",
    "f.close()\n",
    "print(\"Input vector\")\n",
    "print(bow_vec.cpu().numpy())\n",
    "print(\"Probs\")\n",
    "print(probs)\n",
    "print(torch.argmax(probs, dim=1).cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['iter', ' loss'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      1.00      0.80      1855\n",
      "           2       0.00      0.00      0.00       949\n",
      "\n",
      "    accuracy                           0.66      2804\n",
      "   macro avg       0.33      0.50      0.40      2804\n",
      "weighted avg       0.44      0.66      0.53      2804\n",
      "\n",
      "Index(['iter', ' loss'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "bow_cnn_predictions = []\n",
    "original_lables_cnn_bow = []\n",
    "cnn_model.eval()\n",
    "loss_df = pd.read_csv('cnn_class_big_loss_with_padding.csv')\n",
    "print(loss_df.columns)\n",
    "# loss_df.plot('loss')\n",
    "with torch.no_grad():\n",
    "    for index, row in X_test.iterrows():\n",
    "        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n",
    "        probs = cnn_model(bow_vec)\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        bow_cnn_predictions.append(predicted.cpu().numpy()[0])\n",
    "        original_lables_cnn_bow.append(make_target(Y_test['target'][index]).cpu().numpy()[0])\n",
    "        \n",
    "print(classification_report(original_lables_cnn_bow,bow_cnn_predictions))\n",
    "loss_file_name = 'cnn_class_big_loss_with_padding.csv'\n",
    "loss_df = pd.read_csv(loss_file_name)\n",
    "print(loss_df.columns)\n",
    "plt_500_padding_30_epochs = loss_df[' loss'].plot()\n",
    "fig = plt_500_padding_30_epochs.get_figure()\n",
    "fig.savefig('loss_plt_500_padding_30_epochs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test =  pd.read_csv('transferlearning-dl-spring2020/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [user, user, go, home, you, re, drunk, user, m...\n",
      "1                  [user, user, oh, noes, tough, shit]\n",
      "2    [user, canada, doesn, need, another, cuck, we,...\n",
      "3    [user, user, user, it, should, scare, every, a...\n",
      "4    [user, user, user, user, lol, throwing, the, b...\n",
      "5                      [user, user, you, are, correct]\n",
      "6    [user, user, kind, of, like, when, conservativ...\n",
      "7    [the, only, thing, the, democrats, have, is, l...\n",
      "8    [user, user, user, user, user, user, user, use...\n",
      "9    [user, user, user, user, user, user, user, use...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "data_test['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in data_test['text']] \n",
    "print(data_test['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [user, user, go, home, you, re, drunk, user, m...\n",
       "1                   [user, user, oh, noe, tough, shit]\n",
       "2    [user, canada, doesn, need, anoth, cuck, we, a...\n",
       "3    [user, user, user, it, should, scare, everi, a...\n",
       "4    [user, user, user, user, lol, throw, the, bull...\n",
       "5                       [user, user, you, ar, correct]\n",
       "6    [user, user, kind, of, like, when, conserv, wa...\n",
       "7    [the, onli, thing, the, democrat, have, is, ly...\n",
       "8    [user, user, user, user, user, user, user, use...\n",
       "9    [user, user, user, user, user, user, user, use...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "# Get the stemmed_tokens\n",
    "data_test['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in data_test['tokenized_text'] ]\n",
    "data_test['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split Function\n",
    "def split_train_test(top_data_df_small, test_size=1, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[[ 'text',  'stemmed_tokens']], \n",
    "                                                        top_data_df_small['target'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train sentiments\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test sentiments\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Call the train_test_split\n",
    "X_test = data_test[[ 'text',  'stemmed_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "# Use cuda if present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3894\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "size = 500\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "\n",
    "# Function to train word2vec model\n",
    "def make_word2vec_model(top_data_df_small, padding=True, sg=1, min_count=1, size=500, workers=3, window=3):\n",
    "    if  padding:\n",
    "        print(len(top_data_df_small))\n",
    "        temp_df = pd.Series(top_data_df_small['stemmed_tokens']).values\n",
    "        temp_df = list(temp_df)\n",
    "        temp_df.append(['pad'])\n",
    "        word2vec_file =   'word2vec_' + str(size) + '_PAD.model'\n",
    "    else:\n",
    "        temp_df = top_data_df_small['stemmed_tokens']\n",
    "        word2vec_file =   'word2vec_' + str(size) + '.model'\n",
    "    w2v_model = Word2Vec(temp_df, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
    "\n",
    "    w2v_model.save(word2vec_file)\n",
    "    return w2v_model, word2vec_file\n",
    "\n",
    "# Train Word2vec model\n",
    "w2vmodel, word2vec_file = make_word2vec_model(X_test, padding=True, sg=sg, min_count=min_count, size=size, workers=workers, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sen_len = X_test.stemmed_tokens.map(len).max()\n",
    "padding_idx = w2vmodel.wv.vocab['pad'].index\n",
    "def make_word2vec_vector_cnn(sentence):\n",
    "    padded_X = [padding_idx for i in range(max_sen_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        if word not in w2vmodel.wv.vocab:\n",
    "            padded_X[i] = 0\n",
    "            print(word)\n",
    "        else:\n",
    "            padded_X[i] = w2vmodel.wv.vocab[word].index\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "bow_cnn_predictions = []\n",
    "original_lables_cnn_bow = []\n",
    "cnn_model.eval()\n",
    "#loss_df = pd.read_csv('cnn_class_big_loss_with_padding.csv')\n",
    "#print(loss_df.columns)\n",
    "# loss_df.plot('loss')\n",
    "with torch.no_grad():\n",
    "    for index, row in X_test.iterrows():\n",
    "        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n",
    "        probs = cnn_model(bow_vec)\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        bow_cnn_predictions.append(predicted.cpu().numpy()[0])\n",
    "#        original_lables_cnn_bow.append(make_target(Y_test['target'][index]).cpu().numpy()[0])\n",
    "        \n",
    "#print(classification_report(original_lables_cnn_bow,bow_cnn_predictions))\n",
    "#loss_file_name = 'cnn_class_big_loss_with_padding.csv'\n",
    "#loss_df = pd.read_csv(loss_file_name)\n",
    "#print(loss_df.columns)\n",
    "#plt_500_padding_30_epochs = loss_df[' loss'].plot()\n",
    "#fig = plt_500_padding_30_epochs.get_figure()\n",
    "#fig.savefig('loss_plt_500_padding_30_epochs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_cnn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90194</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13384</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12681</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70380</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14726</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>74477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>49845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>47311</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75689</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>84102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10607</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>98992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>53264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>54842</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>48995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>72353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50759</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>93119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>43133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>59537</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32317</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>76680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>80561</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>54190</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>28037</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3866</th>\n",
       "      <td>77430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3867</th>\n",
       "      <td>75815</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>87290</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>99475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>43323</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>37666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>77905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>83400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>84081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875</th>\n",
       "      <td>71649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>20841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>90959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>32598</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>43964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>97745</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>86716</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>21033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>66832</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>28996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3885</th>\n",
       "      <td>64713</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>63482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3887</th>\n",
       "      <td>11132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3888</th>\n",
       "      <td>87416</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3889</th>\n",
       "      <td>90041</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>98824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>95338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3892</th>\n",
       "      <td>67210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893</th>\n",
       "      <td>46552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3894 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  Target\n",
       "0     90194       1\n",
       "1     77444       1\n",
       "2     13384       1\n",
       "3     54920       1\n",
       "4     56117       1\n",
       "5     67757       1\n",
       "6     12681       1\n",
       "7     12609       1\n",
       "8     70380       1\n",
       "9     12108       1\n",
       "10    14726       1\n",
       "11    74477       1\n",
       "12    49845       1\n",
       "13    47311       1\n",
       "14    75689       1\n",
       "15    84102       1\n",
       "16    10607       1\n",
       "17    98992       1\n",
       "18    53264       1\n",
       "19    54842       1\n",
       "20    48995       1\n",
       "21    72353       1\n",
       "22    50759       1\n",
       "23    14574       1\n",
       "24    93119       1\n",
       "25    43133       1\n",
       "26    59537       1\n",
       "27    32317       1\n",
       "28    76680       1\n",
       "29    80561       1\n",
       "...     ...     ...\n",
       "3864  54190       1\n",
       "3865  28037       1\n",
       "3866  77430       1\n",
       "3867  75815       1\n",
       "3868  87290       1\n",
       "3869  99475       1\n",
       "3870  43323       1\n",
       "3871  37666       1\n",
       "3872  77905       1\n",
       "3873  83400       1\n",
       "3874  84081       1\n",
       "3875  71649       1\n",
       "3876  20841       1\n",
       "3877  90959       1\n",
       "3878  32598       1\n",
       "3879  43964       1\n",
       "3880  97745       1\n",
       "3881  86716       1\n",
       "3882  21033       1\n",
       "3883  66832       1\n",
       "3884  28996       1\n",
       "3885  64713       1\n",
       "3886  63482       1\n",
       "3887  11132       1\n",
       "3888  87416       1\n",
       "3889  90041       1\n",
       "3890  98824       1\n",
       "3891  95338       1\n",
       "3892  67210       1\n",
       "3893  46552       1\n",
       "\n",
       "[3894 rows x 2 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['Target'] = bow_cnn_predictions\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('cnn.csv', index=False) # writing data to a CSV file\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for text classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>She should ask few native Americans what their...</td>\n",
       "      <td>1</td>\n",
       "      <td>[she, should, ask, few, native, americans, wha...</td>\n",
       "      <td>[she, should, ask, few, nativ, american, what,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon investigating Chinese employees who are...</td>\n",
       "      <td>0</td>\n",
       "      <td>[amazon, investigating, chinese, employees, wh...</td>\n",
       "      <td>[amazon, investig, chines, employe, who, ar, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62688</td>\n",
       "      <td>Someone should veTaken this piece shit volcano</td>\n",
       "      <td>1</td>\n",
       "      <td>[someone, should, vetaken, this, piece, shit, ...</td>\n",
       "      <td>[someon, should, vetaken, thi, piec, shit, vol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43605</td>\n",
       "      <td>Obama wanted liberals amp illegals move into r...</td>\n",
       "      <td>0</td>\n",
       "      <td>[obama, wanted, liberals, amp, illegals, move,...</td>\n",
       "      <td>[obama, want, liber, amp, illeg, move, into, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97670</td>\n",
       "      <td>Liberals are all Kookoo</td>\n",
       "      <td>1</td>\n",
       "      <td>[liberals, are, all, kookoo]</td>\n",
       "      <td>[liber, ar, all, kookoo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  target  \\\n",
       "0  86426  She should ask few native Americans what their...       1   \n",
       "1  16820  Amazon investigating Chinese employees who are...       0   \n",
       "2  62688     Someone should veTaken this piece shit volcano       1   \n",
       "3  43605  Obama wanted liberals amp illegals move into r...       0   \n",
       "4  97670                            Liberals are all Kookoo       1   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [she, should, ask, few, native, americans, wha...   \n",
       "1  [amazon, investigating, chinese, employees, wh...   \n",
       "2  [someone, should, vetaken, this, piece, shit, ...   \n",
       "3  [obama, wanted, liberals, amp, illegals, move,...   \n",
       "4                       [liberals, are, all, kookoo]   \n",
       "\n",
       "                                      stemmed_tokens  \n",
       "0  [she, should, ask, few, nativ, american, what,...  \n",
       "1  [amazon, investig, chines, employe, who, ar, s...  \n",
       "2  [someon, should, vetaken, thi, piec, shit, vol...  \n",
       "3  [obama, want, liber, amp, illeg, move, into, r...  \n",
       "4                           [liber, ar, all, kookoo]  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9346 entries, 0 to 9345\n",
      "Data columns (total 5 columns):\n",
      "id                9346 non-null int64\n",
      "text              9346 non-null object\n",
      "target            9346 non-null int64\n",
      "tokenized_text    9346 non-null object\n",
      "stemmed_tokens    9346 non-null object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 365.2+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of positive and negative messages')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df.target)\n",
    "plt.xlabel('Label')\n",
    "plt.title('Number of positive and negative messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.text\n",
    "Y = data.target\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 150, 50)           50000     \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 96,337\n",
      "Trainable params: 96,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6355 samples, validate on 1589 samples\n",
      "Epoch 1/10\n",
      "2688/6355 [===========>..................] - ETA: 30s - loss: 0.6564 - accuracy: 0.6533"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-74450c35e6d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n\u001b[0;32m----> 2\u001b[0;31m           validation_split=0.2)\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1402/1402 [==============================] - 5s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Loss: 0.693\n",
      "  Accuracy: 0.530\n"
     ]
    }
   ],
   "source": [
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>[user, user, go, home, you, re, drunk, user, m...</td>\n",
       "      <td>[user, user, go, home, you, re, drunk, user, m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77444</td>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "      <td>[user, user, oh, noes, tough, shit]</td>\n",
       "      <td>[user, user, oh, noe, tough, shit]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13384</td>\n",
       "      <td>@USER Canada doesn’t need another CUCK! We alr...</td>\n",
       "      <td>[user, canada, doesn, need, another, cuck, we,...</td>\n",
       "      <td>[user, canada, doesn, need, anoth, cuck, we, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54920</td>\n",
       "      <td>@USER @USER @USER It should scare every Americ...</td>\n",
       "      <td>[user, user, user, it, should, scare, every, a...</td>\n",
       "      <td>[user, user, user, it, should, scare, everi, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56117</td>\n",
       "      <td>@USER @USER @USER @USER LOL!!!   Throwing the ...</td>\n",
       "      <td>[user, user, user, user, lol, throwing, the, b...</td>\n",
       "      <td>[user, user, user, user, lol, throw, the, bull...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  \\\n",
       "0  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...   \n",
       "1  77444                   @USER @USER Oh noes! Tough shit.   \n",
       "2  13384  @USER Canada doesn’t need another CUCK! We alr...   \n",
       "3  54920  @USER @USER @USER It should scare every Americ...   \n",
       "4  56117  @USER @USER @USER @USER LOL!!!   Throwing the ...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [user, user, go, home, you, re, drunk, user, m...   \n",
       "1                [user, user, oh, noes, tough, shit]   \n",
       "2  [user, canada, doesn, need, another, cuck, we,...   \n",
       "3  [user, user, user, it, should, scare, every, a...   \n",
       "4  [user, user, user, user, lol, throwing, the, b...   \n",
       "\n",
       "                                      stemmed_tokens  Target  \n",
       "0  [user, user, go, home, you, re, drunk, user, m...       0  \n",
       "1                 [user, user, oh, noe, tough, shit]       0  \n",
       "2  [user, canada, doesn, need, anoth, cuck, we, a...       0  \n",
       "3  [user, user, user, it, should, scare, everi, a...       0  \n",
       "4  [user, user, user, user, lol, throw, the, bull...       0  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_test\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3894 entries, 0 to 3893\n",
      "Data columns (total 5 columns):\n",
      "id                3894 non-null int64\n",
      "text              3894 non-null object\n",
      "tokenized_text    3894 non-null object\n",
      "stemmed_tokens    3894 non-null object\n",
      "Target            3894 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 152.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text\n",
    "X_test = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3894, 150)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_sequences_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] >= 0.5:\n",
    "        predicted += [1]\n",
    "    else:\n",
    "        predicted += [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Target'] = predicted\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('lstm.csv', index=False) # writing data to a CSV file\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in range(3894):\n",
    "    pred += [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Target'] = pred\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('random.csv', index=False) # writing data to a CSV file\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-directional RNN for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9346, 200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch   \n",
    "\n",
    "#Reproducing same results\n",
    "SEED = 2019\n",
    "\n",
    "#Torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "#Cuda algorithms\n",
    "torch.backends.cudnn.deterministic = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_is_copy': <weakref at 0x1a30693f98; to 'DataFrame' at 0x1a30555a20>, '_data': BlockManager\n",
      "Items: Index(['id', 'text', 'target'], dtype='object')\n",
      "Axis 1: RangeIndex(start=0, stop=1, step=1)\n",
      "IntBlock: slice(0, 4, 2), 2 x 1, dtype: int64\n",
      "ObjectBlock: slice(1, 2, 1), 1 x 1, dtype: object, '_item_cache': {}}\n"
     ]
    }
   ],
   "source": [
    "#loading custom dataset\n",
    "training_data=data\n",
    "\n",
    "#print preprocessed text\n",
    "print(vars(training_data[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data,X_test, valid_data,Y_test = train_test_split(training_data['text'],training_data['target'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for text classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7476, 200)\n",
      "(25000,)\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 200, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "7476/7476 [==============================] - 120s 16ms/step - loss: 0.6291 - accuracy: 0.6651\n",
      "Epoch 2/10\n",
      "7476/7476 [==============================] - 112s 15ms/step - loss: 0.5440 - accuracy: 0.7289\n",
      "Epoch 3/10\n",
      "7476/7476 [==============================] - 113s 15ms/step - loss: 0.4807 - accuracy: 0.7746\n",
      "Epoch 4/10\n",
      "7476/7476 [==============================] - 125s 17ms/step - loss: 0.4601 - accuracy: 0.7925\n",
      "Epoch 5/10\n",
      "7476/7476 [==============================] - 111s 15ms/step - loss: 0.4495 - accuracy: 0.7968\n",
      "Epoch 6/10\n",
      "7476/7476 [==============================] - 105s 14ms/step - loss: 0.4337 - accuracy: 0.8091\n",
      "Epoch 7/10\n",
      "7476/7476 [==============================] - 106s 14ms/step - loss: 0.4179 - accuracy: 0.8146\n",
      "Epoch 8/10\n",
      "7476/7476 [==============================] - 105s 14ms/step - loss: 0.4056 - accuracy: 0.8234\n",
      "Epoch 9/10\n",
      "7476/7476 [==============================] - 106s 14ms/step - loss: 0.3928 - accuracy: 0.8261\n",
      "Epoch 10/10\n",
      "7476/7476 [==============================] - 112s 15ms/step - loss: 0.3918 - accuracy: 0.8277\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_22_input to have shape (200,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-32a60363e4a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_22_input to have shape (200,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "\n",
    "X = data.text\n",
    "Y = data.target\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2) \n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "X_train = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "#X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "#X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.98%\n"
     ]
    }
   ],
   "source": [
    "#tok.fit_on_texts(X_)\n",
    "sequences = tok.texts_to_sequences(X_test)\n",
    "X_test = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(data_test['text'])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix.shape\n",
    "\n",
    "pred = model.predict(test_sequences_matrix)\n",
    "\n",
    "predicted = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] >= 0.5:\n",
    "        predicted += [1]\n",
    "    else:\n",
    "        predicted += [0]\n",
    "\n",
    "data_test['Target'] = predicted\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('lstm2.csv', index=False) # writing data to a CSV file\n",
    "submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM For text Classification With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7476, 200)\n",
      "(7476,)\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 200, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "7476/7476 [==============================] - 113s 15ms/step - loss: 0.6294 - accuracy: 0.6664\n",
      "Epoch 2/20\n",
      "7476/7476 [==============================] - 109s 15ms/step - loss: 0.5412 - accuracy: 0.7330\n",
      "Epoch 3/20\n",
      "7476/7476 [==============================] - 108s 15ms/step - loss: 0.4781 - accuracy: 0.7824\n",
      "Epoch 4/20\n",
      "7476/7476 [==============================] - 110s 15ms/step - loss: 0.4582 - accuracy: 0.7956\n",
      "Epoch 5/20\n",
      "7476/7476 [==============================] - 114s 15ms/step - loss: 0.4473 - accuracy: 0.8028\n",
      "Epoch 6/20\n",
      "7476/7476 [==============================] - 108s 14ms/step - loss: 0.4334 - accuracy: 0.8089\n",
      "Epoch 7/20\n",
      "7476/7476 [==============================] - 109s 15ms/step - loss: 0.4216 - accuracy: 0.8131\n",
      "Epoch 8/20\n",
      "7476/7476 [==============================] - 122s 16ms/step - loss: 0.4085 - accuracy: 0.8178\n",
      "Epoch 9/20\n",
      "7476/7476 [==============================] - 118s 16ms/step - loss: 0.4059 - accuracy: 0.8202\n",
      "Epoch 10/20\n",
      "7476/7476 [==============================] - 127s 17ms/step - loss: 0.3894 - accuracy: 0.8300\n",
      "Epoch 11/20\n",
      "7476/7476 [==============================] - 114s 15ms/step - loss: 0.3802 - accuracy: 0.8287\n",
      "Epoch 12/20\n",
      "7476/7476 [==============================] - 129s 17ms/step - loss: 0.3721 - accuracy: 0.8356\n",
      "Epoch 13/20\n",
      "7476/7476 [==============================] - 122s 16ms/step - loss: 0.3612 - accuracy: 0.8426\n",
      "Epoch 14/20\n",
      "7476/7476 [==============================] - 123s 16ms/step - loss: 0.3537 - accuracy: 0.8407\n",
      "Epoch 15/20\n",
      "7476/7476 [==============================] - 117s 16ms/step - loss: 0.3493 - accuracy: 0.8487\n",
      "Epoch 16/20\n",
      "7476/7476 [==============================] - 121s 16ms/step - loss: 0.3281 - accuracy: 0.8559\n",
      "Epoch 17/20\n",
      "7476/7476 [==============================] - 116s 15ms/step - loss: 0.3257 - accuracy: 0.8598\n",
      "Epoch 18/20\n",
      "7476/7476 [==============================] - 124s 17ms/step - loss: 0.3156 - accuracy: 0.8666\n",
      "Epoch 19/20\n",
      "7476/7476 [==============================] - 123s 16ms/step - loss: 0.3006 - accuracy: 0.8713\n",
      "Epoch 20/20\n",
      "7476/7476 [==============================] - 113s 15ms/step - loss: 0.3016 - accuracy: 0.8732\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_25_input to have shape (200,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-b855e3d580bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_25_input to have shape (200,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# LSTM with Dropout for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "\n",
    "X = data.text\n",
    "Y = data.target\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2) \n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "X_train = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_len))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.39%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67757</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70380</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14726</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>74477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>49845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>47311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75689</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>84102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>98992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>53264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>54842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>48995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>72353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50759</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>93119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>43133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>59537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32317</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>76680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>80561</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>54190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>28037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3866</th>\n",
       "      <td>77430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3867</th>\n",
       "      <td>75815</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>87290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>99475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>43323</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>37666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>77905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>83400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>84081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875</th>\n",
       "      <td>71649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>20841</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>90959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>32598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>43964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>97745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>86716</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>21033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>66832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>28996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3885</th>\n",
       "      <td>64713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>63482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3887</th>\n",
       "      <td>11132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3888</th>\n",
       "      <td>87416</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3889</th>\n",
       "      <td>90041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>98824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>95338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3892</th>\n",
       "      <td>67210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893</th>\n",
       "      <td>46552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3894 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  Target\n",
       "0     90194       0\n",
       "1     77444       1\n",
       "2     13384       0\n",
       "3     54920       0\n",
       "4     56117       1\n",
       "5     67757       0\n",
       "6     12681       0\n",
       "7     12609       1\n",
       "8     70380       1\n",
       "9     12108       1\n",
       "10    14726       0\n",
       "11    74477       0\n",
       "12    49845       1\n",
       "13    47311       0\n",
       "14    75689       0\n",
       "15    84102       0\n",
       "16    10607       0\n",
       "17    98992       1\n",
       "18    53264       1\n",
       "19    54842       0\n",
       "20    48995       0\n",
       "21    72353       1\n",
       "22    50759       0\n",
       "23    14574       0\n",
       "24    93119       0\n",
       "25    43133       1\n",
       "26    59537       0\n",
       "27    32317       0\n",
       "28    76680       0\n",
       "29    80561       1\n",
       "...     ...     ...\n",
       "3864  54190       0\n",
       "3865  28037       0\n",
       "3866  77430       0\n",
       "3867  75815       1\n",
       "3868  87290       0\n",
       "3869  99475       0\n",
       "3870  43323       0\n",
       "3871  37666       0\n",
       "3872  77905       0\n",
       "3873  83400       0\n",
       "3874  84081       0\n",
       "3875  71649       0\n",
       "3876  20841       0\n",
       "3877  90959       0\n",
       "3878  32598       0\n",
       "3879  43964       0\n",
       "3880  97745       0\n",
       "3881  86716       1\n",
       "3882  21033       1\n",
       "3883  66832       0\n",
       "3884  28996       0\n",
       "3885  64713       0\n",
       "3886  63482       1\n",
       "3887  11132       0\n",
       "3888  87416       0\n",
       "3889  90041       0\n",
       "3890  98824       0\n",
       "3891  95338       1\n",
       "3892  67210       0\n",
       "3893  46552       0\n",
       "\n",
       "[3894 rows x 2 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tok.fit_on_texts(X_)\n",
    "sequences = tok.texts_to_sequences(X_test)\n",
    "X_test = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(data_test['text'])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix.shape\n",
    "\n",
    "pred = model.predict(test_sequences_matrix)\n",
    "\n",
    "predicted = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] >= 0.5:\n",
    "        predicted += [1]\n",
    "    else:\n",
    "        predicted += [0]\n",
    "\n",
    "data_test['Target'] = predicted\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('lstm4.csv', index=False) # writing data to a CSV file\n",
    "submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM and Convolutional Neural Network For Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7476, 200)\n",
      "(7476,)\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_27 (Embedding)     (None, 200, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 200, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "7476/7476 [==============================] - 76s 10ms/step - loss: 0.6315 - accuracy: 0.6663\n",
      "Epoch 2/10\n",
      "7476/7476 [==============================] - 65s 9ms/step - loss: 0.5387 - accuracy: 0.7334\n",
      "Epoch 3/10\n",
      "7476/7476 [==============================] - 66s 9ms/step - loss: 0.4650 - accuracy: 0.7875\n",
      "Epoch 4/10\n",
      "7476/7476 [==============================] - 59s 8ms/step - loss: 0.4393 - accuracy: 0.8039\n",
      "Epoch 5/10\n",
      "7476/7476 [==============================] - 61s 8ms/step - loss: 0.4195 - accuracy: 0.8121\n",
      "Epoch 6/10\n",
      "7476/7476 [==============================] - 61s 8ms/step - loss: 0.3923 - accuracy: 0.8222\n",
      "Epoch 7/10\n",
      "7476/7476 [==============================] - 58s 8ms/step - loss: 0.3610 - accuracy: 0.8404\n",
      "Epoch 8/10\n",
      "7476/7476 [==============================] - 59s 8ms/step - loss: 0.3101 - accuracy: 0.8670\n",
      "Epoch 9/10\n",
      "7476/7476 [==============================] - 59s 8ms/step - loss: 0.2623 - accuracy: 0.8953\n",
      "Epoch 10/10\n",
      "7476/7476 [==============================] - 68s 9ms/step - loss: 0.2010 - accuracy: 0.9240\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_27_input to have shape (200,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-2c043b74a2dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_27_input to have shape (200,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "\n",
    "X = data.text\n",
    "Y = data.target\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2) \n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "X_train = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_len))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.70%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90194</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67757</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14726</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>74477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>49845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>47311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75689</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>84102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>98992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>53264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>54842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>48995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>72353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50759</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>93119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>43133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>59537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32317</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>76680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>80561</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>54190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>28037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3866</th>\n",
       "      <td>77430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3867</th>\n",
       "      <td>75815</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>87290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>99475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>43323</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>37666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>77905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>83400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>84081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875</th>\n",
       "      <td>71649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>20841</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>90959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>32598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>43964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>97745</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>86716</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>21033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>66832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>28996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3885</th>\n",
       "      <td>64713</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>63482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3887</th>\n",
       "      <td>11132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3888</th>\n",
       "      <td>87416</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3889</th>\n",
       "      <td>90041</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>98824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>95338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3892</th>\n",
       "      <td>67210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893</th>\n",
       "      <td>46552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3894 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  Target\n",
       "0     90194       1\n",
       "1     77444       1\n",
       "2     13384       0\n",
       "3     54920       1\n",
       "4     56117       1\n",
       "5     67757       0\n",
       "6     12681       0\n",
       "7     12609       0\n",
       "8     70380       0\n",
       "9     12108       1\n",
       "10    14726       1\n",
       "11    74477       0\n",
       "12    49845       1\n",
       "13    47311       0\n",
       "14    75689       0\n",
       "15    84102       0\n",
       "16    10607       0\n",
       "17    98992       1\n",
       "18    53264       1\n",
       "19    54842       0\n",
       "20    48995       1\n",
       "21    72353       1\n",
       "22    50759       0\n",
       "23    14574       0\n",
       "24    93119       0\n",
       "25    43133       1\n",
       "26    59537       0\n",
       "27    32317       0\n",
       "28    76680       1\n",
       "29    80561       1\n",
       "...     ...     ...\n",
       "3864  54190       0\n",
       "3865  28037       0\n",
       "3866  77430       0\n",
       "3867  75815       1\n",
       "3868  87290       0\n",
       "3869  99475       0\n",
       "3870  43323       1\n",
       "3871  37666       0\n",
       "3872  77905       1\n",
       "3873  83400       0\n",
       "3874  84081       1\n",
       "3875  71649       0\n",
       "3876  20841       0\n",
       "3877  90959       0\n",
       "3878  32598       0\n",
       "3879  43964       0\n",
       "3880  97745       1\n",
       "3881  86716       1\n",
       "3882  21033       1\n",
       "3883  66832       0\n",
       "3884  28996       0\n",
       "3885  64713       1\n",
       "3886  63482       1\n",
       "3887  11132       0\n",
       "3888  87416       0\n",
       "3889  90041       1\n",
       "3890  98824       0\n",
       "3891  95338       1\n",
       "3892  67210       0\n",
       "3893  46552       0\n",
       "\n",
       "[3894 rows x 2 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tok.fit_on_texts(X_)\n",
    "sequences = tok.texts_to_sequences(X_test)\n",
    "X_test = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(data_test['text'])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix.shape\n",
    "\n",
    "pred = model.predict(test_sequences_matrix)\n",
    "\n",
    "predicted = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] >= 0.5:\n",
    "        predicted += [1]\n",
    "    else:\n",
    "        predicted += [0]\n",
    "\n",
    "data_test['Target'] = predicted\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('lstm5.csv', index=False) # writing data to a CSV file\n",
    "submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "X = data.text\n",
    "Y = data.target\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2) \n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "X_train = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(max_len, 1)))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# train LSTM\n",
    "model.fit(X, y, epochs=1, batch_size=1, verbose=2)\n",
    "# evaluate LSTM\n",
    "X,y = get_sequence(n_timesteps)\n",
    "yhat = model.predict_classes(X, verbose=0)\n",
    "for i in range(n_timesteps):\n",
    "\tprint('Expected:', y[0, i], 'Predicted', yhat[0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok.fit_on_texts(X_)\n",
    "sequences = tok.texts_to_sequences(X_test)\n",
    "X_test = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(data_test['text'])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix.shape\n",
    "\n",
    "pred = model.predict(test_sequences_matrix)\n",
    "\n",
    "predicted = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] >= 0.5:\n",
    "        predicted += [1]\n",
    "    else:\n",
    "        predicted += [0]\n",
    "\n",
    "data_test['Target'] = predicted\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('lstm2.csv', index=False) # writing data to a CSV file\n",
    "submission\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
