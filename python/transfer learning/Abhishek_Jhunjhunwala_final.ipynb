{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251      @USER All you need to know is he is empty inside\n",
       "3720                            @USER I. AM. READ. E! URL\n",
       "8376    @USER Go roger I quit watching anyway nfl is o...\n",
       "4471               @USER Yoo our dogs should totally fuck\n",
       "2835              @USER He is a troll.  Not open to facts\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from torchtext import data\n",
    "#from torchtext import datasets\n",
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re # for regular expressions\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "data =  pd.read_csv('transferlearning-dl-spring2020/train.csv')\n",
    "\n",
    "data.head()\n",
    "\n",
    "questions = data['text']\n",
    "labels = data['target']\n",
    "\n",
    "train_data, valid_data, ytrain, yvalid = train_test_split(questions, labels,  \n",
    "                                                          random_state=42, \n",
    "                                                          test_size=0.2)\n",
    "\n",
    "train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97670</td>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  target\n",
       "0  86426  @USER She should ask a few native Americans wh...       1\n",
       "1  16820  Amazon is investigating Chinese employees who ...       0\n",
       "2  62688  @USER Someone should'veTaken\" this piece of sh...       1\n",
       "3  43605  @USER @USER Obama wanted liberals &amp; illega...       0\n",
       "4  97670                  @USER Liberals are all Kookoo !!!       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt    \n",
    "\n",
    "data['text'] = np.vectorize(remove_pattern)(data['text'], \"@[\\w]*\") \n",
    "\n",
    "data['text'] = data['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "data['text'] = data['text'].str.replace('#','')\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "tokenized_tweet = data['text'].apply(lambda x: x.split()) # tokenizing\n",
    "\n",
    "tokenized_tweet.head()\n",
    "\n",
    "len(tokenized_tweet)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_tweet.apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) # stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pre-trained word2vec embeddings created by Google\n",
    "\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "#model.train(tokenized_tweet, total_examples= len(combi['description']), epochs=50)\n",
    "\n",
    "def word_vector_pretrained(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 300))\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector_pretrained(tokenized_tweet[i], 300)\n",
    "    \n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape    \n",
    "\n",
    "X_train, X_test , y_train, y_test = train_test_split(wordvec_df, labels, test_size=0.2, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6691278341911253"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LogisticRegression(random_state = 10)\n",
    "lreg.fit(X_train, y_train) # training the model\n",
    "\n",
    "prediction = lreg.predict(X_test) # predicting on the validation set\n",
    "\n",
    "prediction_int = prediction.astype(np.int)\n",
    "\n",
    "f1_score(y_test, prediction,average = 'macro') # calculating f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test =  pd.read_csv('transferlearning-dl-spring2020/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['text'] = np.vectorize(remove_pattern)(data_test['text'], \"@[\\w]*\") \n",
    "\n",
    "data_test['text'] = data_test['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "data_test['text'] = data_test['text'].str.replace('#','')\n",
    "\n",
    "data_test['text'] = data_test['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "tokenized_tweet_test = data_test['text'].apply(lambda x: x.split()) # tokenizing\n",
    "\n",
    "tokenized_tweet_test.head()\n",
    "\n",
    "len(tokenized_tweet_test)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_tweet_test.apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) # stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3894, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wordvec_arrays_test = np.zeros((len(tokenized_tweet_test), 300))\n",
    "\n",
    "for i in range(len(tokenized_tweet_test)):\n",
    "    wordvec_arrays_test[i,:] = word_vector_pretrained(tokenized_tweet_test[i], 300)\n",
    "    \n",
    "wordvec_df_test = pd.DataFrame(wordvec_arrays_test)\n",
    "wordvec_df_test.shape    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = np.array(wordvec_df_test)\n",
    "\n",
    "prediction1 = lreg.predict(X_test1) # predicting on the validation set\n",
    "\n",
    "prediction_int1 = prediction1.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_int1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Target'] = prediction_int1\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('pretrained_word2vec.csv', index=False) # writing data to a CSV file\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuned word2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9346, 50)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=50, # desired no. of features/independent variables \n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34)\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(tokenized_tweet), epochs=200)\n",
    "\n",
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 50))\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 50)\n",
    "    \n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5291078614322713"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(wordvec_df, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "lreg = LogisticRegression(random_state = 20)\n",
    "lreg.fit(X_train, y_train) # training the model\n",
    "\n",
    "prediction = lreg.predict(X_test) # predicting on the validation set\n",
    "\n",
    "prediction_int = prediction.astype(np.int)\n",
    "\n",
    "f1_score(y_test, prediction,average = 'macro') # calculating f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for text classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_data_df_small = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [she, should, ask, few, native, americans, wha...\n",
      "1    [amazon, investigating, chinese, employees, wh...\n",
      "2    [someone, should, vetaken, this, piece, shit, ...\n",
      "3    [obama, wanted, liberals, amp, illegals, move,...\n",
      "4                         [liberals, are, all, kookoo]\n",
      "5    [was, literally, just, talking, about, this, l...\n",
      "6                                [buy, more, icecream]\n",
      "7             [not, fault, you, support, gun, control]\n",
      "8    [what, the, difference, between, kavanaugh, an...\n",
      "9    [you, are, lying, corrupt, traitor, nobody, wa...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "top_data_df_small['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in top_data_df_small['text']] \n",
    "print(top_data_df_small['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [she, should, ask, few, nativ, american, what,...\n",
       "1    [amazon, investig, chines, employe, who, ar, s...\n",
       "2    [someon, should, vetaken, thi, piec, shit, vol...\n",
       "3    [obama, want, liber, amp, illeg, move, into, r...\n",
       "4                             [liber, ar, all, kookoo]\n",
       "5    [wa, liter, just, talk, about, thi, lol, all, ...\n",
       "6                                [bui, more, icecream]\n",
       "7             [not, fault, you, support, gun, control]\n",
       "8    [what, the, differ, between, kavanaugh, and, o...\n",
       "9    [you, ar, ly, corrupt, traitor, nobodi, want, ...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "# Get the stemmed_tokens\n",
    "top_data_df_small['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in top_data_df_small['tokenized_text'] ]\n",
    "top_data_df_small['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for Train sentiments\n",
      "0    4365\n",
      "1    2177\n",
      "Name: target, dtype: int64\n",
      "Value counts for Test sentiments\n",
      "0    1855\n",
      "1     949\n",
      "Name: target, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "   index                                               text  \\\n",
      "0   8030                          least you are honest lmao   \n",
      "1   3722                         All that shit was hers URL   \n",
      "2   3522                         Cutie connection authentic   \n",
      "3   4232  What interview Evanne honest and right from th...   \n",
      "4   1678  Warriorscoach fundraising for gun control Brad...   \n",
      "\n",
      "                                      stemmed_tokens  \n",
      "0                     [least, you, ar, honest, lmao]  \n",
      "1                    [all, that, shit, wa, her, url]  \n",
      "2                           [cuti, connect, authent]  \n",
      "3  [what, interview, evann, honest, and, right, f...  \n",
      "4  [warriorscoach, fundrais, for, gun, control, b...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split Function\n",
    "def split_train_test(top_data_df_small, test_size=0.3, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[[ 'text',  'stemmed_tokens']], \n",
    "                                                        top_data_df_small['target'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train sentiments\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test sentiments\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Call the train_test_split\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "# Use cuda if present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9346\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "size = 500\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "\n",
    "# Function to train word2vec model\n",
    "def make_word2vec_model(top_data_df_small, padding=True, sg=1, min_count=1, size=500, workers=3, window=3):\n",
    "    if  padding:\n",
    "        print(len(top_data_df_small))\n",
    "        temp_df = pd.Series(top_data_df_small['stemmed_tokens']).values\n",
    "        temp_df = list(temp_df)\n",
    "        temp_df.append(['pad'])\n",
    "        word2vec_file =   'word2vec_' + str(size) + '_PAD.model'\n",
    "    else:\n",
    "        temp_df = top_data_df_small['stemmed_tokens']\n",
    "        word2vec_file =   'word2vec_' + str(size) + '.model'\n",
    "    w2v_model = Word2Vec(temp_df, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
    "\n",
    "    w2v_model.save(word2vec_file)\n",
    "    return w2v_model, word2vec_file\n",
    "\n",
    "# Train Word2vec model\n",
    "w2vmodel, word2vec_file = make_word2vec_model(top_data_df_small, padding=True, sg=sg, min_count=min_count, size=size, workers=workers, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the output tensor\n",
    "def make_target(label):\n",
    "    if label == -1:\n",
    "        return torch.tensor([0], dtype=torch.long, device=device)\n",
    "    elif label == 0:\n",
    "        return torch.tensor([1], dtype=torch.long, device=device)\n",
    "    else:\n",
    "        return torch.tensor([2], dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 500\n",
    "NUM_FILTERS = 10\n",
    "import gensim\n",
    "\n",
    "class CnnTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n",
    "        super(CnnTextClassifier, self).__init__()\n",
    "        w2vmodel = gensim.models.KeyedVectors.load( 'word2vec_500_PAD.model')\n",
    "        weights = w2vmodel.wv\n",
    "        # With pretrained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors), padding_idx=w2vmodel.wv.vocab['pad'].index)\n",
    "        # Without pretrained embeddings\n",
    "        # self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE], padding=(window_size - 1, 0))\n",
    "                                   for window_size in window_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # [B, T, E]\n",
    "\n",
    "        # Apply a convolution + max_pool layer for each window size\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        xs = []\n",
    "        for conv in self.convs:\n",
    "            x2 = torch.tanh(conv(x))\n",
    "            x2 = torch.squeeze(x2, -1)\n",
    "            x2 = F.max_pool1d(x2, x2.size(2))\n",
    "            xs.append(x2)\n",
    "        x = torch.cat(xs, 2)\n",
    "\n",
    "        # FC\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sen_len = top_data_df_small.stemmed_tokens.map(len).max()\n",
    "padding_idx = w2vmodel.wv.vocab['pad'].index\n",
    "def make_word2vec_vector_cnn(sentence):\n",
    "    padded_X = [padding_idx for i in range(max_sen_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        if word not in w2vmodel.wv.vocab:\n",
    "            padded_X[i] = 0\n",
    "            print(word)\n",
    "        else:\n",
    "            padded_X[i] = w2vmodel.wv.vocab[word].index\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n",
      "Epoch ran :1\n",
      "Epoch2\n",
      "Epoch ran :2\n",
      "Epoch3\n",
      "Epoch ran :3\n",
      "Epoch4\n",
      "Epoch ran :4\n",
      "Epoch5\n",
      "Epoch ran :5\n",
      "Epoch6\n",
      "Epoch ran :6\n",
      "Epoch7\n",
      "Epoch ran :7\n",
      "Epoch8\n",
      "Epoch ran :8\n",
      "Epoch9\n",
      "Epoch ran :9\n",
      "Epoch10\n",
      "Epoch ran :10\n",
      "Epoch11\n",
      "Epoch ran :11\n",
      "Epoch12\n",
      "Epoch ran :12\n",
      "Epoch13\n",
      "Epoch ran :13\n",
      "Epoch14\n",
      "Epoch ran :14\n",
      "Epoch15\n",
      "Epoch ran :15\n",
      "Epoch16\n",
      "Epoch ran :16\n",
      "Epoch17\n",
      "Epoch ran :17\n",
      "Epoch18\n",
      "Epoch ran :18\n",
      "Epoch19\n",
      "Epoch ran :19\n",
      "Epoch20\n",
      "Epoch ran :20\n",
      "Epoch21\n",
      "Epoch ran :21\n",
      "Epoch22\n",
      "Epoch ran :22\n",
      "Epoch23\n",
      "Epoch ran :23\n",
      "Epoch24\n",
      "Epoch ran :24\n",
      "Epoch25\n",
      "Epoch ran :25\n",
      "Epoch26\n",
      "Epoch ran :26\n",
      "Epoch27\n",
      "Epoch ran :27\n",
      "Epoch28\n",
      "Epoch ran :28\n",
      "Epoch29\n",
      "Epoch ran :29\n",
      "Epoch30\n",
      "Epoch ran :30\n",
      "Input vector\n",
      "[[  235    13    16     0   928   172   358   244     7     7 10887 10887\n",
      "  10887 10887 10887 10887 10887 10887 10887 10887 10887 10887 10887 10887\n",
      "  10887 10887 10887 10887 10887 10887 10887 10887 10887 10887 10887 10887\n",
      "  10887 10887 10887 10887 10887 10887 10887 10887 10887 10887 10887 10887\n",
      "  10887]]\n",
      "Probs\n",
      "tensor([[1.1411e-11, 9.5633e-01, 4.3666e-02]], grad_fn=<SoftmaxBackward>)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 3\n",
    "VOCAB_SIZE = len(w2vmodel.wv.vocab)\n",
    "\n",
    "cnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\n",
    "cnn_model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "num_epochs = 30\n",
    "\n",
    "# Open the file for writing loss\n",
    "loss_file_name = 'cnn_class_big_loss_with_padding.csv'\n",
    "f = open(loss_file_name,'w')\n",
    "f.write('iter, loss')\n",
    "f.write('\\n')\n",
    "losses = []\n",
    "cnn_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch\" + str(epoch + 1))\n",
    "    train_loss = 0\n",
    "    for index, row in X_train.iterrows():\n",
    "        # Clearing the accumulated gradients\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        # Make the bag of words vector for stemmed tokens \n",
    "        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n",
    "       \n",
    "        # Forward pass to get output\n",
    "        probs = cnn_model(bow_vec)\n",
    "\n",
    "        # Get the target label\n",
    "        target = make_target(Y_train['target'][index])\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = loss_function(probs, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # if index == 0:\n",
    "    #     continue\n",
    "    print(\"Epoch ran :\"+ str(epoch+1))\n",
    "    f.write(str((epoch+1)) + \",\" + str(train_loss / len(X_train)))\n",
    "    f.write('\\n')\n",
    "    train_loss = 0\n",
    "\n",
    "torch.save(cnn_model, 'cnn_big_model_500_with_padding.pth')\n",
    "\n",
    "f.close()\n",
    "print(\"Input vector\")\n",
    "print(bow_vec.cpu().numpy())\n",
    "print(\"Probs\")\n",
    "print(probs)\n",
    "print(torch.argmax(probs, dim=1).cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['iter', ' loss'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      1.00      0.80      1855\n",
      "           2       0.00      0.00      0.00       949\n",
      "\n",
      "    accuracy                           0.66      2804\n",
      "   macro avg       0.33      0.50      0.40      2804\n",
      "weighted avg       0.44      0.66      0.53      2804\n",
      "\n",
      "Index(['iter', ' loss'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "bow_cnn_predictions = []\n",
    "original_lables_cnn_bow = []\n",
    "cnn_model.eval()\n",
    "loss_df = pd.read_csv('cnn_class_big_loss_with_padding.csv')\n",
    "print(loss_df.columns)\n",
    "# loss_df.plot('loss')\n",
    "with torch.no_grad():\n",
    "    for index, row in X_test.iterrows():\n",
    "        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n",
    "        probs = cnn_model(bow_vec)\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        bow_cnn_predictions.append(predicted.cpu().numpy()[0])\n",
    "        original_lables_cnn_bow.append(make_target(Y_test['target'][index]).cpu().numpy()[0])\n",
    "        \n",
    "print(classification_report(original_lables_cnn_bow,bow_cnn_predictions))\n",
    "loss_file_name = 'cnn_class_big_loss_with_padding.csv'\n",
    "loss_df = pd.read_csv(loss_file_name)\n",
    "print(loss_df.columns)\n",
    "plt_500_padding_30_epochs = loss_df[' loss'].plot()\n",
    "fig = plt_500_padding_30_epochs.get_figure()\n",
    "fig.savefig('loss_plt_500_padding_30_epochs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test =  pd.read_csv('transferlearning-dl-spring2020/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [user, user, go, home, you, re, drunk, user, m...\n",
      "1                  [user, user, oh, noes, tough, shit]\n",
      "2    [user, canada, doesn, need, another, cuck, we,...\n",
      "3    [user, user, user, it, should, scare, every, a...\n",
      "4    [user, user, user, user, lol, throwing, the, b...\n",
      "5                      [user, user, you, are, correct]\n",
      "6    [user, user, kind, of, like, when, conservativ...\n",
      "7    [the, only, thing, the, democrats, have, is, l...\n",
      "8    [user, user, user, user, user, user, user, use...\n",
      "9    [user, user, user, user, user, user, user, use...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "data_test['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in data_test['text']] \n",
    "print(data_test['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [user, user, go, home, you, re, drunk, user, m...\n",
       "1                   [user, user, oh, noe, tough, shit]\n",
       "2    [user, canada, doesn, need, anoth, cuck, we, a...\n",
       "3    [user, user, user, it, should, scare, everi, a...\n",
       "4    [user, user, user, user, lol, throw, the, bull...\n",
       "5                       [user, user, you, ar, correct]\n",
       "6    [user, user, kind, of, like, when, conserv, wa...\n",
       "7    [the, onli, thing, the, democrat, have, is, ly...\n",
       "8    [user, user, user, user, user, user, user, use...\n",
       "9    [user, user, user, user, user, user, user, use...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "# Get the stemmed_tokens\n",
    "data_test['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in data_test['tokenized_text'] ]\n",
    "data_test['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split Function\n",
    "def split_train_test(top_data_df_small, test_size=1, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[[ 'text',  'stemmed_tokens']], \n",
    "                                                        top_data_df_small['target'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train sentiments\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test sentiments\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Call the train_test_split\n",
    "X_test = data_test[[ 'text',  'stemmed_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "# Use cuda if present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3894\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "size = 500\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "\n",
    "# Function to train word2vec model\n",
    "def make_word2vec_model(top_data_df_small, padding=True, sg=1, min_count=1, size=500, workers=3, window=3):\n",
    "    if  padding:\n",
    "        print(len(top_data_df_small))\n",
    "        temp_df = pd.Series(top_data_df_small['stemmed_tokens']).values\n",
    "        temp_df = list(temp_df)\n",
    "        temp_df.append(['pad'])\n",
    "        word2vec_file =   'word2vec_' + str(size) + '_PAD.model'\n",
    "    else:\n",
    "        temp_df = top_data_df_small['stemmed_tokens']\n",
    "        word2vec_file =   'word2vec_' + str(size) + '.model'\n",
    "    w2v_model = Word2Vec(temp_df, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
    "\n",
    "    w2v_model.save(word2vec_file)\n",
    "    return w2v_model, word2vec_file\n",
    "\n",
    "# Train Word2vec model\n",
    "w2vmodel, word2vec_file = make_word2vec_model(X_test, padding=True, sg=sg, min_count=min_count, size=size, workers=workers, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sen_len = X_test.stemmed_tokens.map(len).max()\n",
    "padding_idx = w2vmodel.wv.vocab['pad'].index\n",
    "def make_word2vec_vector_cnn(sentence):\n",
    "    padded_X = [padding_idx for i in range(max_sen_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        if word not in w2vmodel.wv.vocab:\n",
    "            padded_X[i] = 0\n",
    "            print(word)\n",
    "        else:\n",
    "            padded_X[i] = w2vmodel.wv.vocab[word].index\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "bow_cnn_predictions = []\n",
    "original_lables_cnn_bow = []\n",
    "cnn_model.eval()\n",
    "#loss_df = pd.read_csv('cnn_class_big_loss_with_padding.csv')\n",
    "#print(loss_df.columns)\n",
    "# loss_df.plot('loss')\n",
    "with torch.no_grad():\n",
    "    for index, row in X_test.iterrows():\n",
    "        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n",
    "        probs = cnn_model(bow_vec)\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        bow_cnn_predictions.append(predicted.cpu().numpy()[0])\n",
    "#        original_lables_cnn_bow.append(make_target(Y_test['target'][index]).cpu().numpy()[0])\n",
    "        \n",
    "#print(classification_report(original_lables_cnn_bow,bow_cnn_predictions))\n",
    "#loss_file_name = 'cnn_class_big_loss_with_padding.csv'\n",
    "#loss_df = pd.read_csv(loss_file_name)\n",
    "#print(loss_df.columns)\n",
    "#plt_500_padding_30_epochs = loss_df[' loss'].plot()\n",
    "#fig = plt_500_padding_30_epochs.get_figure()\n",
    "#fig.savefig('loss_plt_500_padding_30_epochs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Target'] = bow_cnn_predictions\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('cnn.csv', index=False) # writing data to a CSV file\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for text classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>She should ask few native Americans what their...</td>\n",
       "      <td>1</td>\n",
       "      <td>[she, should, ask, few, native, americans, wha...</td>\n",
       "      <td>[she, should, ask, few, nativ, american, what,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon investigating Chinese employees who are...</td>\n",
       "      <td>0</td>\n",
       "      <td>[amazon, investigating, chinese, employees, wh...</td>\n",
       "      <td>[amazon, investig, chines, employe, who, ar, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62688</td>\n",
       "      <td>Someone should veTaken this piece shit volcano</td>\n",
       "      <td>1</td>\n",
       "      <td>[someone, should, vetaken, this, piece, shit, ...</td>\n",
       "      <td>[someon, should, vetaken, thi, piec, shit, vol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43605</td>\n",
       "      <td>Obama wanted liberals amp illegals move into r...</td>\n",
       "      <td>0</td>\n",
       "      <td>[obama, wanted, liberals, amp, illegals, move,...</td>\n",
       "      <td>[obama, want, liber, amp, illeg, move, into, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97670</td>\n",
       "      <td>Liberals are all Kookoo</td>\n",
       "      <td>1</td>\n",
       "      <td>[liberals, are, all, kookoo]</td>\n",
       "      <td>[liber, ar, all, kookoo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  target  \\\n",
       "0  86426  She should ask few native Americans what their...       1   \n",
       "1  16820  Amazon investigating Chinese employees who are...       0   \n",
       "2  62688     Someone should veTaken this piece shit volcano       1   \n",
       "3  43605  Obama wanted liberals amp illegals move into r...       0   \n",
       "4  97670                            Liberals are all Kookoo       1   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [she, should, ask, few, native, americans, wha...   \n",
       "1  [amazon, investigating, chinese, employees, wh...   \n",
       "2  [someone, should, vetaken, this, piece, shit, ...   \n",
       "3  [obama, wanted, liberals, amp, illegals, move,...   \n",
       "4                       [liberals, are, all, kookoo]   \n",
       "\n",
       "                                      stemmed_tokens  \n",
       "0  [she, should, ask, few, nativ, american, what,...  \n",
       "1  [amazon, investig, chines, employe, who, ar, s...  \n",
       "2  [someon, should, vetaken, thi, piec, shit, vol...  \n",
       "3  [obama, want, liber, amp, illeg, move, into, r...  \n",
       "4                           [liber, ar, all, kookoo]  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9346 entries, 0 to 9345\n",
      "Data columns (total 5 columns):\n",
      "id                9346 non-null int64\n",
      "text              9346 non-null object\n",
      "target            9346 non-null int64\n",
      "tokenized_text    9346 non-null object\n",
      "stemmed_tokens    9346 non-null object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 365.2+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of positive and negative messages')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df.target)\n",
    "plt.xlabel('Label')\n",
    "plt.title('Number of positive and negative messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-directional RNN for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9346, 200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch   \n",
    "\n",
    "#Reproducing same results\n",
    "SEED = 2019\n",
    "\n",
    "#Torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "#Cuda algorithms\n",
    "torch.backends.cudnn.deterministic = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_is_copy': <weakref at 0x1a30693f98; to 'DataFrame' at 0x1a30555a20>, '_data': BlockManager\n",
      "Items: Index(['id', 'text', 'target'], dtype='object')\n",
      "Axis 1: RangeIndex(start=0, stop=1, step=1)\n",
      "IntBlock: slice(0, 4, 2), 2 x 1, dtype: int64\n",
      "ObjectBlock: slice(1, 2, 1), 1 x 1, dtype: object, '_item_cache': {}}\n"
     ]
    }
   ],
   "source": [
    "#loading custom dataset\n",
    "training_data=data\n",
    "\n",
    "#print preprocessed text\n",
    "print(vars(training_data[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data,X_test, valid_data,Y_test = train_test_split(training_data['text'],training_data['target'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for text classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7476, 200)\n",
      "(25000,)\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 200, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "7476/7476 [==============================] - 120s 16ms/step - loss: 0.6291 - accuracy: 0.6651\n",
      "Epoch 2/10\n",
      "7476/7476 [==============================] - 112s 15ms/step - loss: 0.5440 - accuracy: 0.7289\n",
      "Epoch 3/10\n",
      "7476/7476 [==============================] - 113s 15ms/step - loss: 0.4807 - accuracy: 0.7746\n",
      "Epoch 4/10\n",
      "7476/7476 [==============================] - 125s 17ms/step - loss: 0.4601 - accuracy: 0.7925\n",
      "Epoch 5/10\n",
      "7476/7476 [==============================] - 111s 15ms/step - loss: 0.4495 - accuracy: 0.7968\n",
      "Epoch 6/10\n",
      "7476/7476 [==============================] - 105s 14ms/step - loss: 0.4337 - accuracy: 0.8091\n",
      "Epoch 7/10\n",
      "7476/7476 [==============================] - 106s 14ms/step - loss: 0.4179 - accuracy: 0.8146\n",
      "Epoch 8/10\n",
      "7476/7476 [==============================] - 105s 14ms/step - loss: 0.4056 - accuracy: 0.8234\n",
      "Epoch 9/10\n",
      "7476/7476 [==============================] - 106s 14ms/step - loss: 0.3928 - accuracy: 0.8261\n",
      "Epoch 10/10\n",
      "7476/7476 [==============================] - 112s 15ms/step - loss: 0.3918 - accuracy: 0.8277\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_22_input to have shape (200,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-32a60363e4a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_22_input to have shape (200,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "\n",
    "X = data.text\n",
    "Y = data.target\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2) \n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "X_train = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "#X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "#X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.98%\n"
     ]
    }
   ],
   "source": [
    "#tok.fit_on_texts(X_)\n",
    "sequences = tok.texts_to_sequences(X_test)\n",
    "X_test = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(data_test['text'])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix.shape\n",
    "\n",
    "pred = model.predict(test_sequences_matrix)\n",
    "\n",
    "predicted = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] >= 0.5:\n",
    "        predicted += [1]\n",
    "    else:\n",
    "        predicted += [0]\n",
    "\n",
    "data_test['Target'] = predicted\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('lstm2.csv', index=False) # writing data to a CSV file\n",
    "submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM For text Classification With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7476, 200)\n",
      "(7476,)\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 200, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "7476/7476 [==============================] - 113s 15ms/step - loss: 0.6294 - accuracy: 0.6664\n",
      "Epoch 2/20\n",
      "7476/7476 [==============================] - 109s 15ms/step - loss: 0.5412 - accuracy: 0.7330\n",
      "Epoch 3/20\n",
      "7476/7476 [==============================] - 108s 15ms/step - loss: 0.4781 - accuracy: 0.7824\n",
      "Epoch 4/20\n",
      "7476/7476 [==============================] - 110s 15ms/step - loss: 0.4582 - accuracy: 0.7956\n",
      "Epoch 5/20\n",
      "7476/7476 [==============================] - 114s 15ms/step - loss: 0.4473 - accuracy: 0.8028\n",
      "Epoch 6/20\n",
      "7476/7476 [==============================] - 108s 14ms/step - loss: 0.4334 - accuracy: 0.8089\n",
      "Epoch 7/20\n",
      "7476/7476 [==============================] - 109s 15ms/step - loss: 0.4216 - accuracy: 0.8131\n",
      "Epoch 8/20\n",
      "7476/7476 [==============================] - 122s 16ms/step - loss: 0.4085 - accuracy: 0.8178\n",
      "Epoch 9/20\n",
      "6336/7476 [========================>.....] - ETA: 17s - loss: 0.4055 - accuracy: 0.8212"
     ]
    }
   ],
   "source": [
    "# LSTM with Dropout for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "\n",
    "X = data.text\n",
    "Y = data.target\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2) \n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "X_train = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_len))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok.fit_on_texts(X_)\n",
    "sequences = tok.texts_to_sequences(X_test)\n",
    "X_test = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(data_test['text'])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix.shape\n",
    "\n",
    "pred = model.predict(test_sequences_matrix)\n",
    "\n",
    "predicted = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] >= 0.5:\n",
    "        predicted += [1]\n",
    "    else:\n",
    "        predicted += [0]\n",
    "\n",
    "data_test['Target'] = predicted\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('lstm2.csv', index=False) # writing data to a CSV file\n",
    "submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM and Convolutional Neural Network For Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "X_train,X_test,y_train,y_test = train_test_split(data['text'],data['target'],test_size=0.2) \n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok.fit_on_texts(X_)\n",
    "sequences = tok.texts_to_sequences(X_test)\n",
    "X_test = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(data_test['text'])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix.shape\n",
    "\n",
    "pred = model.predict(test_sequences_matrix)\n",
    "\n",
    "predicted = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] >= 0.5:\n",
    "        predicted += [1]\n",
    "    else:\n",
    "        predicted += [0]\n",
    "\n",
    "data_test['Target'] = predicted\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('lstm2.csv', index=False) # writing data to a CSV file\n",
    "submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "# create a sequence classification instance\n",
    "def get_sequence(n_timesteps):\n",
    "\t# create a sequence of random numbers in [0,1]\n",
    "\tX = array([random() for _ in range(n_timesteps)])\n",
    "\t# calculate cut-off value to change class values\n",
    "\tlimit = n_timesteps/4.0\n",
    "\t# determine the class outcome for each item in cumulative sequence\n",
    "\ty = array([0 if x < limit else 1 for x in cumsum(X)])\n",
    "\t# reshape input and output data to be suitable for LSTMs\n",
    "\tX = X.reshape(1, n_timesteps, 1)\n",
    "\ty = y.reshape(1, n_timesteps, 1)\n",
    "\treturn X, y\n",
    "\n",
    "# define problem properties\n",
    "n_timesteps = 10\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(n_timesteps, 1)))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# train LSTM\n",
    "for epoch in range(1000):\n",
    "\t# generate new random sequence\n",
    "\tX,y = get_sequence(n_timesteps)\n",
    "\t# fit model for one epoch on this sequence\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=1, verbose=2)\n",
    "# evaluate LSTM\n",
    "X,y = get_sequence(n_timesteps)\n",
    "yhat = model.predict_classes(X, verbose=0)\n",
    "for i in range(n_timesteps):\n",
    "\tprint('Expected:', y[0, i], 'Predicted', yhat[0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok.fit_on_texts(X_)\n",
    "sequences = tok.texts_to_sequences(X_test)\n",
    "X_test = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(data_test['text'])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "\n",
    "test_sequences_matrix.shape\n",
    "\n",
    "pred = model.predict(test_sequences_matrix)\n",
    "\n",
    "predicted = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] >= 0.5:\n",
    "        predicted += [1]\n",
    "    else:\n",
    "        predicted += [0]\n",
    "\n",
    "data_test['Target'] = predicted\n",
    "submission = data_test[['id','Target']]\n",
    "submission.to_csv('lstm2.csv', index=False) # writing data to a CSV file\n",
    "submission\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
